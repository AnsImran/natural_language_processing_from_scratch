{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "739b2615",
   "metadata": {
    "colab_type": "text",
    "id": "C7oVbe_pLr3C"
   },
   "source": [
    "# Assignment 3 - Named Entity Recognition (NER)\n",
    "\n",
    "Welcome to the third programming assignment of Course 3. In this assignment, you will learn to build more complicated models with pytorch. By completing this assignment, you will be able to: \n",
    "\n",
    "- Design the architecture of a neural network, train it, and test it. \n",
    "- Process features and represents them\n",
    "- Understand word padding\n",
    "- Implement LSTMs\n",
    "- Test with your own sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a1cb8",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#0)\n",
    "- [1 - Exploring the Data](#1)\n",
    "    - [1.1 - Importing the Data](#1-1)\n",
    "    - [1.2 - Data Generator](#1-2)\n",
    "\t\t- [Exercise 1 - data_generator (UNQ_C1)](#ex-1)\n",
    "- [2 - Building the Model](#2)\n",
    "\t- [Exercise 2 - NER (UNQ_C2)](#ex-2)\n",
    "- [3 - Train the Model](#3)\n",
    "    - [3.1 - Training the Model](#3-1)\n",
    "        - [Exercise 3 - train_model (UNQ_C3)](#ex-3)\n",
    "- [4 - Compute Accuracy](#4)\n",
    "\t- [Exercise 4 - evaluate_prediction (UNQ_C4)](#ex-4)\n",
    "- [5 - Testing with your Own Sentence](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd50714",
   "metadata": {
    "colab_type": "text",
    "id": "ftT-5-yynCtl"
   },
   "source": [
    "<a name=\"0\"></a>\n",
    "## Introduction\n",
    "\n",
    "We first start by defining named entity recognition (NER). NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. \n",
    "\n",
    "For example:\n",
    "\n",
    "<img src = 'images/ner.png' width=\"width\" height=\"height\" style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "Is labeled as follows: \n",
    "\n",
    "- French: geopolitical entity\n",
    "- Morocco: geographic entity \n",
    "- Christmas: time indicator\n",
    "\n",
    "Everything else that is labeled with an `O` is not considered to be a named entity. In this assignment, you will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. Then, you will load in the exact version of your model, which was trained for a longer period of time. You could then evaluate the trained version of your model to get 96% accuracy! Finally, you will be able to test your named entity recognition system with your own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18107908",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "JEY_jlQQR9SP",
    "outputId": "825f37fd-cf03-483a-a6b1-3d70da6f70f1"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from utils import get_params, get_vocab\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "rnd.seed(33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c611eca",
   "metadata": {
    "colab_type": "text",
    "id": "_PpjG5MuLr3F"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Exploring the Data\n",
    "\n",
    "We will be using a dataset from Kaggle, which we will preprocess for you. The original data consists of four columns: the sentence number, the word, the part of speech of the word, and the tags.  A few tags you might expect to see are: \n",
    "\n",
    "* geo: geographical entity\n",
    "* org: organization\n",
    "* per: person \n",
    "* gpe: geopolitical entity\n",
    "* tim: time indicator\n",
    "* art: artifact\n",
    "* eve: event\n",
    "* nat: natural phenomenon\n",
    "* O: filler word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3df805f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "-Jur1JnXnCtr",
    "outputId": "88584ab6-0a15-489b-db5b-eb474e129c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "ORIGINAL DATA:\n",
      "     Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "# display original kaggle data\n",
    "data         = pd.read_csv(\"data/ner_dataset.csv\", encoding = \"ISO-8859-1\") \n",
    "train_sents  = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "\n",
    "print('SENTENCE:', train_sents)\n",
    "print('SENTENCE LABEL:', train_labels)\n",
    "print('ORIGINAL DATA:\\n', data.head(5))\n",
    "del(data, train_sents, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191340e",
   "metadata": {
    "colab_type": "text",
    "id": "xoH6yBWVfzTb"
   },
   "source": [
    "<a name=\"1-1\"></a>\n",
    "### 1.1 - Importing the Data\n",
    "\n",
    "In this part, we will import the preprocessed data and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "104b9adc",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UauHjIKHWC0N"
   },
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "\n",
    "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60043923",
   "metadata": {
    "colab_type": "text",
    "id": "mcQi6EmWnCty"
   },
   "source": [
    "`vocab` is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a `<PAD>` token. \n",
    "\n",
    "When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic `<PAD>` token to fill all the empty spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6c073e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sm2P8y7zNgdU",
    "outputId": "1f1d077a-7c57-42df-fb67-48dd00da39ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab[\"the\"]: 9\n",
      "padded token: 35180\n"
     ]
    }
   ],
   "source": [
    "# vocab translates from a word to a unique number\n",
    "print('vocab[\"the\"]:', vocab[\"the\"])\n",
    "# Pad token\n",
    "print('padded token:', vocab['<PAD>'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4339c",
   "metadata": {
    "colab_type": "text",
    "id": "IY6BTBjunCt1"
   },
   "source": [
    "The `tag_map` is a dictionary that maps the tags that you could have to numbers. Run the cell below to see the possible classes you will be predicting. The prepositions in the tags mean:\n",
    "* I: Token is inside an entity.\n",
    "* B: Token begins an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab4a13d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZzMamaPcQXWP",
    "outputId": "4f04364c-a88c-4e77-f8bb-9bfcb422d5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"
     ]
    }
   ],
   "source": [
    "print(tag_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd673f5",
   "metadata": {
    "colab_type": "text",
    "id": "3F1sUP_MnCt5"
   },
   "source": [
    "If you had the sentence \n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "The tags would look like:\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "where you would have three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon's last name to the sentence:\n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e5dd8cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "xM9B_Rwxd01i",
    "outputId": "db098ed6-4351-41f7-cfdb-e45dd3798ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is tag_map 17\n",
      "\n",
      "Num of vocabulary words:                 35181\n",
      "\n",
      "The training size is                     33570\n",
      "\n",
      "The validation size is                   7194\n",
      "\n",
      "An example of the first sentence is      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
      "\n",
      "An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Exploring information about the data\n",
    "print('The number of outputs is tag_map', len(tag_map))\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "g_vocab_size = len(vocab)\n",
    "print()\n",
    "\n",
    "print(f\"Num of vocabulary words:                 {g_vocab_size}\")\n",
    "print()\n",
    "\n",
    "print('The training size is                    ', t_size)\n",
    "print()\n",
    "\n",
    "print('The validation size is                  ', v_size)\n",
    "print()\n",
    "\n",
    "print('An example of the first sentence is     ', t_sentences[0])\n",
    "print()\n",
    "\n",
    "print('An example of its corresponding label is', t_labels[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd174c25",
   "metadata": {
    "colab_type": "text",
    "id": "IPd5a-4_nCt8"
   },
   "source": [
    "So you can see that we have already encoded each sentence into a tensor by converting it into a number. We also have 16 possible tags (excluding the '0' tag), as shown in the tag map.\n",
    "\n",
    "\n",
    "<a name=\"1-2\"></a>\n",
    "### 1.2 - Data Generator\n",
    "\n",
    "In python, a generator is a function that behaves like an iterator. It returns the next item in a pre-defined sequence. Here is a [link](https://wiki.python.org/moin/Generators) to review python generators. \n",
    "\n",
    "In many AI applications it is very useful to have a data generator. You will now implement a data generator for our NER application.\n",
    "\n",
    "<a name=\"ex-1\"></a>\n",
    "### Exercise 1 - data_generator\n",
    "\n",
    "**Instructions:** Implement a data generator function that takes in `batch_size, x, y, pad, shuffle` where $x$ is a large list of sentences, and $y$ is a list of the tags associated with those sentences and pad is a pad value. Return a subset of those inputs in a tuple of two arrays `(X,Y)`. \n",
    "\n",
    "`X` and `Y` are arrays of dimension (`batch_size, max_len`), where `max_len` is the length of the longest sentence *in that batch*. You will pad the `X` and `Y` examples with the pad argument. If `shuffle=True`, the data will be traversed in a random order.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "Use this code as an outer loop\n",
    "```\n",
    "while True:  \n",
    "...  \n",
    "yield((X,Y))  \n",
    "```\n",
    "\n",
    "so your data generator runs continuously. Within that loop, define two `for` loops:  \n",
    "\n",
    "1. The first stores temporal lists of the data samples to be included in the batch, and finds the maximum length of the sentences contained in it.\n",
    "\n",
    "2. The second one moves the elements from the temporal list into NumPy arrays pre-filled with pad values.\n",
    "\n",
    "There are three features useful for defining this generator:\n",
    "\n",
    "1. The NumPy `full` function to fill the NumPy arrays with a pad value. See [full function documentation](https://numpy.org/doc/1.18/reference/generated/numpy.full.html).\n",
    "\n",
    "2. Tracking the current location in the incoming lists of sentences. Generators variables hold their values between invocations, so we create an `index` variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the `index` to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.  \n",
    "\n",
    "3. Since `batch_size` and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the `index` to 0. We can re-shuffle the list of indexes to produce different batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231e90b2",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP7zQC8knCt_"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\n",
    "    '''\n",
    "      Input: \n",
    "        batch_size - integer describing the batch size\n",
    "        x - list containing sentences where words are represented as integers\n",
    "        y - list containing tags associated with the sentences\n",
    "        shuffle - Shuffle the data order\n",
    "        pad - an integer representing a pad character\n",
    "        verbose - Print information during runtime\n",
    "      Output:\n",
    "        a tuple containing 2 elements:\n",
    "        X - np.ndarray of dim (batch_size, max_len) of padded sentences\n",
    "        Y - np.ndarray of dim (batch_size, max_len) of tags associated with the sentences in X\n",
    "    '''\n",
    "    \n",
    "    # count the number of lines in data_lines\n",
    "    num_lines = len(x)\n",
    "    \n",
    "    # create an array with the indexes of data_lines that can be shuffled\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle the indexes if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    index = 0 # tracks current location in x, y\n",
    "    while True:\n",
    "        buffer_x = [0] * batch_size # Temporal array to store the raw x data for this batch\n",
    "        buffer_y = [0] * batch_size # Temporal array to store the raw y data for this batch\n",
    "                \n",
    "  ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        \n",
    "        # Copy into the temporal buffers the sentences in x[index] \n",
    "        # along with their corresponding labels y[index]\n",
    "        # Find maximum length of sentences in x[index] for this batch. \n",
    "        # Reset the index if we reach the end of the data set, and shuffle the indexes if needed.\n",
    "        max_len = 0 \n",
    "        for i in range(batch_size):\n",
    "             # if the index is greater than or equal to the number of lines in x\n",
    "            if index >= num_lines:\n",
    "                # then reset the index to 0\n",
    "                index = 0\n",
    "                # re-shuffle the indexes if shuffle is set to True\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(lines_index)\n",
    "            \n",
    "            # The current position is obtained using `lines_index[index]`\n",
    "            # Store the x value at the current position into the buffer_x\n",
    "            buffer_x[i] = x[lines_index[index]]\n",
    "            \n",
    "            # Store the y value at the current position into the buffer_y\n",
    "            buffer_y[i] = y[lines_index[index]]\n",
    "            \n",
    "            lenx = len(x[lines_index[index]]) #length of current x[]\n",
    "            if lenx > max_len:\n",
    "                max_len = lenx #max_len tracks longest x[]\n",
    "            \n",
    "            # increment index by one\n",
    "            index += 1\n",
    "\n",
    "\n",
    "        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\n",
    "        X = np.full((batch_size, max_len), pad)\n",
    "        Y = np.full((batch_size, max_len), pad)\n",
    "\n",
    "        # copy values from lists to NumPy arrays. Use the buffered values\n",
    "        for i in range(batch_size):\n",
    "            # get the example (sentence as a tensor)\n",
    "            # in `buffer_x` at the `i` index\n",
    "            x_i = buffer_x[i]\n",
    "            \n",
    "            # similarly, get the example's labels\n",
    "            # in `buffer_y` at the `i` index\n",
    "            y_i = buffer_y[i]\n",
    "            \n",
    "            # Walk through each word in x_i\n",
    "            for j in range(len(x_i)):\n",
    "                # store the word in x_i at position j into X\n",
    "                X[i, j] = x_i[j]\n",
    "                \n",
    "                # store the label in y_i at position j into Y\n",
    "                Y[i, j] = y_i[j]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "        if verbose: print(\"index=\", index)\n",
    "        yield((X,Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5220c803",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "s3fwE3PMhOW4",
    "outputId": "12022225-1498-4acb-c0c8-cd02b71c091d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 5\n",
      "index= 2\n",
      "Y1.shape:  (5, 30)\n",
      "Y2.shape:  (5, 30)\n",
      "\n",
      "X2.shape:  (5, 30)\n",
      "X2.shape:  (5, 30)\n",
      "\n",
      "\n",
      "X1[0][:]:  [    0     1     2     3     4     5     6     7     8     9    10    11\n",
      "    12    13    14     9    15     1    16    17    18    19    20    21\n",
      " 35180 35180 35180 35180 35180 35180] \n",
      "\n",
      " Y1[0][:]:  [    0     0     0     0     0     0     1     0     0     0     0     0\n",
      "     1     0     0     0     0     0     2     0     0     0     0     0\n",
      " 35180 35180 35180 35180 35180 35180]\n"
     ]
    }
   ],
   "source": [
    "batch_size     = 5\n",
    "mini_sentences = t_sentences[0: 8]\n",
    "mini_labels    = t_labels[0: 8]\n",
    "\n",
    "dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[\"<PAD>\"], shuffle=False, verbose=True)\n",
    "\n",
    "X1, Y1 = next(dg)  # index= 5 mean k next iteration par kis index say data uthana shuru krna hay is nay\n",
    "X2, Y2 = next(dg)\n",
    "\n",
    "print('Y1.shape: ', Y1.shape)\n",
    "#print()\n",
    "print('Y2.shape: ', Y2.shape)\n",
    "print()\n",
    "print('X2.shape: ', X2.shape)\n",
    "#print()\n",
    "print('X2.shape: ', X2.shape)\n",
    "print('\\n')\n",
    "\n",
    "print('X1[0][:]: ', X1[0][:], \"\\n\\n\", 'Y1[0][:]: ', Y1[0][:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193d063",
   "metadata": {
    "colab_type": "text",
    "id": "W-qWOhFunCuH"
   },
   "source": [
    "**Expected output:**   \n",
    "```\n",
    "index= 5\n",
    "index= 2\n",
    "(5, 30) (5, 30) (5, 30) (5, 30)\n",
    "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
    "    12    13    14     9    15     1    16    17    18    19    20    21\n",
    " 35180 35180 35180 35180 35180 35180] \n",
    " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
    "     1     0     0     0     0     0     2     0     0     0     0     0\n",
    " 35180 35180 35180 35180 35180 35180]  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f3ef6",
   "metadata": {
    "colab_type": "text",
    "id": "4SWxKhkVLr3P"
   },
   "source": [
    "## Building the Model\n",
    "\n",
    "You will now implement the model that will be able to determining the tags of sentences like the following:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "<img src = 'images/ner1.png' width=\"width\" height=\"height\" style=\"width:500px;height:150px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The model architecture will be as follows: \n",
    "\n",
    "<img src = 'images/ner2.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>\n",
    "\n",
    "\n",
    "Concretely, your inputs will be sentences represented as tensors that are fed to a model with:\n",
    "\n",
    "* An Embedding layer,\n",
    "* A LSTM layer\n",
    "* A Dense layer\n",
    "* A log softmax layer.\n",
    "\n",
    "Good news! We won't make you implement the LSTM cell drawn above. You will be in charge of the overall architecture of the model.\n",
    "\n",
    "\n",
    "### Exercise - Named Entity Recognition - NER\n",
    "\n",
    "**Instructions:** Implement the initialization step and the forward function of your Named Entity Recognition system.  \n",
    "Please utilize help function e.g. `help(nn.Linear)` for more information on a layer\n",
    "   \n",
    "\n",
    "-  nn.Embedding: Initializes the embedding. In this case it is the dimension of the model by the size of the vocabulary. \n",
    "    - `nn.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
    "    \n",
    "\n",
    "-  nn.LSTM: LSTM layer. \n",
    "    - `LSTM(n_units)` Builds an LSTM layer with hidden state and cell sizes equal to `n_units`. In trax, `n_units` should be equal to the size of the embeddings `d_feature`.\n",
    "\n",
    "\n",
    "\n",
    "-  tl.Linear: A dense layer.\n",
    "    - `nn.Linear(n_units)`: The parameter `n_units` is the number of units chosen for this dense layer.  \n",
    "\n",
    "\n",
    "- F.log_softmax(): Log of the output probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "57b31f97-6a78-4913-851d-70f8e576e248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER(\n",
      "  (embedding): Embedding(35181, 50)\n",
      "  (lstm): LSTM(50, 128, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=17, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NER(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NER, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=35181, embedding_dim=50)\n",
    "        \n",
    "        self.lstm      = nn.LSTM(input_size=50, hidden_size=64, batch_first=True, num_layers=3)\n",
    "        \n",
    "        self.fc        = nn.Linear(in_features=64, out_features=17)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, memmory_cell):                                                             ############################\n",
    "        embedded_output = self.embedding(x)\n",
    "\n",
    "        lstm_output, (hidden, memmory_cell) = self.lstm(embedded_output, (hidden, memmory_cell))            ############################\n",
    "\n",
    "        linear_output = self.fc(lstm_output)\n",
    "\n",
    "        return F.log_softmax(linear_output, dim=-1), hidden, memmory_cell                                   ############################\n",
    "\n",
    "\n",
    "\n",
    "model = NER().to(device)########################\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a8dcd-68df-49b3-a501-31dbdc994ab8",
   "metadata": {},
   "source": [
    "## Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "799eec33-f64a-43be-97aa-f652f5a4122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss function\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d461f-417d-4889-bc69-335fdaaaa9a4",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba8dce8e-c7e3-4cd2-98c5-9d68f166a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed for reproducibility and testing\n",
    "rnd.seed(33)\n",
    "batch_size = 64\n",
    "\n",
    "# Create training data\n",
    "train_generator = data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], shuffle=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf377b13-35f4-46fd-bd20-89c371315109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((64, 40), (64, 40))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_inputs, batch_of_labels = next(train_generator)\n",
    "batch_of_inputs.shape, batch_of_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebe4ba2a-b6db-4f02-8cf0-09851fcd8359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     1,     0,     0,\n",
       "           0,     0,     0,     1,     0,     0,     0,     0,     0,\n",
       "           2,     0,     0,     0,     0,     0, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_labels[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce702139-940d-43a2-8c3a-cc1f59574a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35180"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<PAD>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11de9caf-4041-4b4a-b50e-c4a2230854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_of_labels = np.where(batch_of_labels == 35180, 0, batch_of_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce77e4b7-d1a8-46f5-859b-1846fa299b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_labels[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e32ac-387e-450d-901d-d8075f627eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67dd2be4-5fd9-4d67-893d-5e644b27219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((64, 41), (64, 41))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_inputs, batch_of_labels = next(train_generator)\n",
    "batch_of_inputs.shape, batch_of_labels.shape\n",
    "\n",
    "# note length of sequence change ho rahi hay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bfbaebd-4e48-47aa-9fde-9be668176fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524.53125, 524.53125)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_sentences)/64, len(t_sentences)/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b1a20-0929-4690-bb57-8f3ac8b4e4ab",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3794bfaa-3a59-483f-a44f-56c0ef0ae9f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  250\n",
      "loss:  0.4959373499055308\n",
      "\n",
      "i:  500\n",
      "loss:  0.3845687680496665\n",
      "\n",
      "Epoch [1/10], Average Loss: 0.37729055792829835\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.20794603537278347\n",
      "\n",
      "i:  500\n",
      "loss:  0.1934807835343831\n",
      "\n",
      "Epoch [2/10], Average Loss: 0.19203686054101915\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.14636622301017146\n",
      "\n",
      "i:  500\n",
      "loss:  0.13590730039659374\n",
      "\n",
      "Epoch [3/10], Average Loss: 0.1347477340950372\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.10432186244849664\n",
      "\n",
      "i:  500\n",
      "loss:  0.0981997159084755\n",
      "\n",
      "Epoch [4/10], Average Loss: 0.09750957326687335\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.08253520520856655\n",
      "\n",
      "i:  500\n",
      "loss:  0.07867030660341123\n",
      "\n",
      "Epoch [5/10], Average Loss: 0.07831733099790467\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.07075413137644648\n",
      "\n",
      "i:  500\n",
      "loss:  0.06707745376312566\n",
      "\n",
      "Epoch [6/10], Average Loss: 0.06685994191895533\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.06173164769265044\n",
      "\n",
      "i:  500\n",
      "loss:  0.05843341918643601\n",
      "\n",
      "Epoch [7/10], Average Loss: 0.0582440593935011\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.05444776151047285\n",
      "\n",
      "i:  500\n",
      "loss:  0.05122090261169299\n",
      "\n",
      "Epoch [8/10], Average Loss: 0.05116926897458817\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.048125687919824245\n",
      "\n",
      "i:  500\n",
      "loss:  0.04528893498767041\n",
      "\n",
      "Epoch [9/10], Average Loss: 0.045374202046245905\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i:  250\n",
      "loss:  0.042821628887547675\n",
      "\n",
      "i:  500\n",
      "loss:  0.04030800107197312\n",
      "\n",
      "Epoch [10/10], Average Loss: 0.040509820147328957\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 64\n",
    "# resetting training gen\n",
    "train_generator = data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], shuffle=False, verbose=False)\n",
    "\n",
    "n_epochs = 10\n",
    "steps_per_epoch = len(t_sentences)/batch_size\n",
    "\n",
    "\n",
    "# setting up initial hidden and cell state\n",
    "num_layers  = 3\n",
    "batch_size  = 64\n",
    "hidden_size = 128\n",
    "# Optionally, create initial hidden and cell states\n",
    "hidden_state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "memmory_cell = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "# hidden_state.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()                  # agar evaluation accuracy etc bhi check kr rahay hotay tu i guess model ko dobara is tarin mode main dalna zroori tha\n",
    "    total_loss=0.0\n",
    "\n",
    "    i=0\n",
    "    \n",
    "    for batch_of_inputs, batch_of_labels in train_generator:\n",
    "        i+=1\n",
    "        batch_of_labels = np.where(batch_of_labels==35180, 0, batch_of_labels)\n",
    "        batch_of_inputs, batch_of_labels = torch.from_numpy(batch_of_inputs).long(), torch.from_numpy(batch_of_labels).long()\n",
    "        batch_of_inputs, batch_of_labels = batch_of_inputs.to(device), batch_of_labels.to(device)######################################\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        log_softmax_predictions, _, _ = model(batch_of_inputs, hidden_state, memmory_cell)     # OR log_softmax_predictions, hidden_state, memmory_cell\n",
    "        \n",
    "        # Reshaping the outputs and targets to match\n",
    "        num_classes             =  log_softmax_predictions.size(-1)\n",
    "        log_softmax_predictions =  log_softmax_predictions.view(-1, num_classes)\n",
    "        batch_of_labels         =  batch_of_labels.view(-1)\n",
    "\n",
    "        # Computing the loss\n",
    "        loss = criterion(log_softmax_predictions, batch_of_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 250 == 0:\n",
    "            print('i: ',i)\n",
    "            print('loss: ', total_loss / (i + 1))\n",
    "            print()\n",
    "        if i >= 525:\n",
    "            break\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / (i + 1)\n",
    "    print(f\"Epoch [{epoch + 1}/{n_epochs}], Average Loss: {avg_loss}\")\n",
    "    print('\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "827dacd9-7bd3-4028-b840-98cee6e95b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'named_entity_recogniser.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e409a50-80ea-40d9-b47d-af21ecb3b101",
   "metadata": {},
   "source": [
    "# Checking the inputs and ouputs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45f148-bcb2-4980-b1ed-65d6c97897fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the device (GPU if available, otherwise CPU)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# class NER(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NER, self).__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(num_embeddings=35181, embedding_dim=50)\n",
    "        \n",
    "#         self.lstm      = nn.LSTM(input_size=50, hidden_size=34, batch_first=True, )\n",
    "        \n",
    "#         self.fc        = nn.Linear(in_features=34, out_features=17)\n",
    "\n",
    "\n",
    "#     def forward(self, x, hidden, memmory_cell):                                                             ############################\n",
    "#         embedded_output = self.embedding(x)\n",
    "\n",
    "#         lstm_output, (hidden, memmory_cell) = self.lstm(embedded_output, (hidden, memmory_cell))            ############################\n",
    "\n",
    "#         linear_output = self.fc(lstm_output)\n",
    "\n",
    "#         return F.log_softmax(linear_output, dim=-1), hidden, memmory_cell                                   ############################\n",
    "\n",
    "\n",
    "\n",
    "# model = NER().to(device)########################\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca886e6b-b305-4fa5-b7ab-1e9f18ac9c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequences:  torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# generating random sequnces\n",
    "\n",
    "vocab_size=35181\n",
    "torch.manual_seed(0)\n",
    "input_sequences = torch.randint(0, vocab_size, (64, 10))\n",
    "print('input_sequences: ', input_sequences.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a874437a-bb19-4217-87f5-e6fa1d2ebd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_out:  torch.Size([64, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# embedding layer\n",
    "embedding_ = nn.Embedding(num_embeddings=vocab_size, embedding_dim=50)\n",
    "embed_out_ = embedding_(input_sequences)\n",
    "\n",
    "print('embed_out: ', embed_out_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e84ef164-a7a5-46d0-8dcb-db56bbb244e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([64, 10, 34])\n",
      "hn shape: torch.Size([1, 64, 34])\n",
      "cn shape: torch.Size([1, 64, 34])\n"
     ]
    }
   ],
   "source": [
    "# GRU layer\n",
    "\n",
    "# Defining the LSTM parameters\n",
    "input_size  = 50  # Number of features in the input\n",
    "hidden_size = 34 # Number of features in the hidden state\n",
    "num_layers  = 1   # Number of recurrent layers\n",
    "\n",
    "\n",
    "# Create an LSTM layer\n",
    "lstm_ = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "\n",
    "# Optionally, create initial hidden and cell states\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "\n",
    "# Forward pass through the LSTM\n",
    "lstm_out_, (hn, cn) = lstm_(embed_out_, (h0, c0))             ##############################################################################################\n",
    "\n",
    "# Print the shapes of the outputs\n",
    "print(\"Output shape:\", lstm_out_.shape)  # Output shape: (batch_size, seq_len, num_directions * hidden_size)\n",
    "print(\"hn shape:\", hn.shape)          # hn shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "print(\"cn shape:\", cn.shape)          # cn shape: (num_layers * num_directions, batch_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fcf384e-24cc-4a44-94c1-77e05e0c736a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[0,0,:] = 0\n",
    "hn[0,0,:]\n",
    "\n",
    "# so ye qaabil e taghaiyyur hain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27648a-0087-47ec-899a-5564e2aef8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea42627a-5606-4b6f-89c3-9a079cefc673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 17])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully connected or dense layer\n",
    "fc_ = nn.Linear(in_features=34, out_features=17)\n",
    "fc_out_ = fc_(lstm_out_)\n",
    "fc_out_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d454b9d1-566c-485f-84ba-21873610ecd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 17])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking log softmax\n",
    "softmax_out_ = F.softmax(fc_out_, dim=-1)\n",
    "softmax_out_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "54b80527-bdcd-4ae2-ae40-d8049aa5e166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0550, 0.0607, 0.0531, 0.0604, 0.0530, 0.0571, 0.0703, 0.0601, 0.0656,\n",
       "        0.0552, 0.0693, 0.0488, 0.0514, 0.0572, 0.0641, 0.0561, 0.0627],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_out_[0,0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "712cfba8-aa1b-41a8-a6d4-da9d5c256b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(softmax_out_[0,0,:])\n",
    "# perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b272c-253e-4ebd-bd54-145de9941d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f4bc929-f370-4bb3-b5cf-8b2ccea4aba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 17])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing the same using the whole model directly\n",
    "# does the model gives the same output? yes!\n",
    "a,b,c = model(input_sequences, h0, c0)\n",
    "a.shape\n",
    "#perfect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370705a-b247-4942-946f-f5fab9c9fd0c",
   "metadata": {},
   "source": [
    "# Accuracy on Test Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d6585-6c7a-4368-958a-f86e56377a04",
   "metadata": {},
   "source": [
    "### data generator and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b51d188-3868-423f-b532-fa847c0d19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = len(v_sentences)\n",
    "test_generator = data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], shuffle=False, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7ce4ba5-d883-44f3-81af-b55585f5d6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7194, 7194)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_sentences), len(v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "32b1e533-25a1-480b-8662-508dac31861b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7194, 73), (7194, 73))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs, val_labels = next(test_generator)\n",
    "val_inputs.shape, val_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5bd298f9-9c2b-4b68-8ecb-844a64516586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1020,    68,  5092,    50,     9, 29845,  1677, 18327,  1033,\n",
       "            9,  4452,    13,   522, 29846,    45, 10314,   223,  6582,\n",
       "           21, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180]),\n",
       " array([    1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     5,\n",
       "            0, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs[0,:], val_labels[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba98d242-92b3-41b7-95a2-7306114b441f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.where(val_labels != 35180, 1, val_labels)\n",
    "mask[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ea6ffba1-3bae-4b00-b1ab-a1a310b4c19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.where(mask == 35180, 0, mask)\n",
    "mask[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "901c25d0-8b7c-4959-a8d7-d91f134dbc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     5,\n",
       "           0, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c211cd4-9cfb-4b59-a7ed-15c0f917edd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2925ddaf-119d-4697-a89b-69a8e8fb2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs, val_labels = torch.from_numpy(val_inputs).long().to(device), torch.from_numpy(val_labels).long().to(device)\n",
    "mask = torch.from_numpy(mask).long().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029cedb-ef7b-4e45-a0fc-c4f952e3072f",
   "metadata": {},
   "source": [
    "### making predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "807db7c8-dbed-4aec-af29-2a7067ec68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up initial hidden and cell state\n",
    "num_layers  = 3\n",
    "batch_size  = len(v_sentences)\n",
    "hidden_size = 64\n",
    "# Optionally, create initial hidden and cell states\n",
    "hidden_state = torch.zeros(num_layers, batch_size, hidden_size).to(device)####################################\n",
    "memmory_cell = torch.zeros(num_layers, batch_size, hidden_size).to(device)#####################################\n",
    "# hidden_state.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c86e916f-9463-441d-a07f-c7a9bb58bcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7194, 73, 17])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions, _, _ = model(val_inputs, hidden_state, memmory_cell)\n",
    "predictions.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003d8d3-d1d3-47f7-b1dd-b986c1a5e44b",
   "metadata": {},
   "source": [
    "#### reshaping predictions, getting class indices from logsoftmax probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d92a3e76-1c2b-4d2d-ac6d-9f43f797fda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4,  8,  5,  9, 12,  9,  2,  3,  4, 16,  8,  1,  5,  2,  3, 15, 10],\n",
       "         [ 1, 12, 14, 11, 12,  3, 13,  2,  4, 14, 16,  8, 16, 13,  4, 11, 10],\n",
       "         [13, 14, 12,  1,  0,  6, 10,  6,  2, 15,  1,  2,  1,  9, 14, 10,  2]],\n",
       "\n",
       "        [[14, 10, 16, 15,  3, 12,  8, 11,  5, 13,  4, 16, 16,  3, 15, 15, 16],\n",
       "         [15,  5, 15,  7,  9, 10,  4,  9,  9,  1,  6,  8, 10,  6, 14, 10, 13],\n",
       "         [ 0, 16,  9, 14, 10,  7,  8, 10, 16, 13, 13, 12,  4, 10,  0,  3,  3]]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing argmax on random data\n",
    "a = torch.randint(0, 17, (2, 3, 17))\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "81d4fc7a-8e79-4e13-b6bb-1e34a715b63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 10,  9],\n",
       "        [ 2,  0,  1]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing argmax on random data\n",
    "torch.argmax(a, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b4796ddb-9fe2-4fb7-a6ad-a91bd877d311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7194, 73])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_predictions = torch.argmax(predictions, dim=-1)\n",
    "reduced_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fd6fbc34-d7c8-47db-b596-8f63d734b58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 1, 4, 0, 7, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0], device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looing at data\n",
    "reduced_predictions[1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "efbced7a-398a-4bb9-8260-2e1ee3fddf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,     0,     0,     0,     0,     0,     0,     1,     4,     0,\n",
       "            0,     0,     0,     7,     0,     0,     0,     0,     0,     0,\n",
       "            8,     0, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180], device='cuda:0')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6d001-b2a4-471c-8eca-8d728a1e51f1",
   "metadata": {},
   "source": [
    "### Accuracy using for loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a59b92b0-a2cb-4935-8e94-2dd0cc6c1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_correct_preds = []\n",
    "list_of_lengths_of_sequences = []\n",
    "\n",
    "for i in range(len(val_labels)):\n",
    "    \n",
    "    length_of_current_sequence      = torch.sum(mask[i,:])\n",
    " \n",
    "    current_number_of_correct_preds = torch.sum( reduced_predictions[i,: length_of_current_sequence] == val_labels[i, :length_of_current_sequence] )\n",
    "    \n",
    "    list_of_lengths_of_sequences.append(length_of_current_sequence)\n",
    "    list_of_correct_preds.append(current_number_of_correct_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a75b7eb9-7d41-4085-942f-be7e5820002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18, 20,  9,  ..., 12,  9, 20]),\n",
       " tensor([19, 22, 10,  ..., 12,  9, 20]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_preds        = torch.tensor(list_of_correct_preds)\n",
    "lengths_of_sequences = torch.tensor(list_of_lengths_of_sequences)\n",
    "correct_preds, lengths_of_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679c2db-dd00-4cde-a318-7a617ce9da41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "108158ae-ace9-4dcf-a75d-4faee3ffd1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9511)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = torch.sum(correct_preds)/torch.sum(lengths_of_sequences)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470db61-b359-4493-b82e-4e601ff3edd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c381e0f6-184f-4910-943c-df01176c43bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19, device='cuda:0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mask[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4f060fda-e395-4e70-b73a-e643153ae341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     5,     0, 35180],\n",
       "        device='cuda:0'),\n",
       " tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[0, :torch.sum(mask[0,:])+1], val_labels[0, :torch.sum(mask[0,:])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5fe552bc-570b-45e0-ab41-2deae3d95590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.tensor([1,2,3,4,5])==torch.tensor([1,2,3,4,6]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1c9a01ce-0671-4917-8a57-bcc66497eb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18, device='cuda:0')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_length=19\n",
    "torch.sum( reduced_predictions[0,: current_length] == val_labels[0, :current_length] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f46ee4-8af6-4920-94c0-d2eb0994a1ce",
   "metadata": {},
   "source": [
    "### Accuracy using matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "407c512c-0b7d-42b9-9539-35003c2cb105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     0,     0,  ..., 35180, 35180, 35180],\n",
       "        [    1,     0,     0,  ..., 35180, 35180, 35180],\n",
       "        [    5,     0,     0,  ..., 35180, 35180, 35180],\n",
       "        ...,\n",
       "        [    0,     0,     0,  ..., 35180, 35180, 35180],\n",
       "        [    0,     0,     0,  ..., 35180, 35180, 35180],\n",
       "        [    0,     0,     0,  ..., 35180, 35180, 35180]], device='cuda:0')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what do val_labels look like?\n",
    "val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bfb6d5b6-bc0d-42e1-a263-6bcc84a553d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what do preds look like\n",
    "reduced_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4fef4097-a905-49fa-bf78-89cc09039a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([19, 22, 10,  ..., 12,  9, 20], device='cuda:0'), torch.Size([7194]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of each sequence without padding\n",
    "lengths_of_sequences = torch.sum(mask, dim=-1)\n",
    "lengths_of_sequences, lengths_of_sequences.shape #perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "96ea2145-7172-4fb7-b558-82982373ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20, device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is it working fine on a random sequence?\n",
    "torch.sum(reduced_predictions[1,:] == val_labels[1,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "995ce439-6cfe-4349-81b7-04d310e00c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9511, device='cuda:0')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally accuracy\n",
    "accuracy = torch.sum(reduced_predictions == val_labels)/torch.sum(lengths_of_sequences)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04afbc",
   "metadata": {
    "colab_type": "text",
    "id": "b2FEleAFLr3r"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Testing with your Own Sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fb9d9",
   "metadata": {
    "colab_type": "text",
    "id": "EOeTPAx_Lr3t"
   },
   "source": [
    "Below, you can test it out with your own sentence! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b2972e7f-afe3-40ad-b186-d87fffb656fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "def predict(sentence, model, vocab, tag_map):\n",
    "    # Tokenize the sentence and convert tokens to their corresponding indices\n",
    "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "    \n",
    "    # Create a tensor with the appropriate shape and fill it with the indices\n",
    "    batch_data = torch.ones((1, len(s)), dtype=torch.long).to(device)\n",
    "    batch_data[0, :] = torch.tensor(s).to(device)\n",
    "    \n",
    "    # Pass the tensor through the model\n",
    "    with torch.no_grad():\n",
    "        output, _, _ = model(batch_data, hidden_state, memmory_cell)\n",
    "    \n",
    "    # Get the indices of the maximum values along the last dimension\n",
    "    outputs = torch.argmax(output, dim=2)\n",
    "    \n",
    "    # Convert indices to labels using the tag_map\n",
    "    labels = list(tag_map.keys())\n",
    "    pred = [labels[idx] for idx in outputs[0]]\n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d95d736-7910-4deb-be8a-e596d2559744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter B-per\n",
      "Navarro, I-per\n",
      "White B-org\n",
      "House I-org\n",
      "U.S, B-per\n",
      "Sunday B-tim\n",
      "morning I-tim\n",
      "White B-org\n",
      "House I-org\n",
      "coronavirus B-org\n",
      "fall, B-gpe\n",
      "wouldn’t B-per\n"
     ]
    }
   ],
   "source": [
    "# setting up initial hidden and cell state\n",
    "num_layers  = 3\n",
    "batch_size  = 1\n",
    "hidden_size = 128\n",
    "# Optionally, create initial hidden and cell states\n",
    "hidden_state = torch.zeros(num_layers, batch_size, hidden_size).to(device)####################################\n",
    "memmory_cell = torch.zeros(num_layers, batch_size, hidden_size).to(device)#####################################\n",
    "# hidden_state.shape\n",
    "\n",
    "\n",
    "# Try the output for the introduction example\n",
    "#sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
    "#sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
    "\n",
    "# New york times news:\n",
    "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "predictions = predict(sentence, model, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc256b95",
   "metadata": {
    "colab_type": "text",
    "id": "NHYbSnYKnCu6"
   },
   "source": [
    "**Expected Results**\n",
    "\n",
    "```\n",
    "Peter B-per\n",
    "Navarro, I-per\n",
    "White B-org\n",
    "House I-org\n",
    "Sunday B-tim\n",
    "morning I-tim\n",
    "White B-org\n",
    "House I-org\n",
    "coronavirus B-tim\n",
    "fall, B-tim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73288ba-84a0-429e-b326-57d3b4725686",
   "metadata": {},
   "source": [
    "### their output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c9e17d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "vLZCHoiULr3u",
    "outputId": "fab815fd-0472-4eaf-968a-abff1f5cfff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter B-per\n",
      "Navarro, I-per\n",
      "White B-org\n",
      "House I-org\n",
      "Sunday B-tim\n",
      "morning I-tim\n",
      "White B-org\n",
      "House I-org\n",
      "coronavirus B-tim\n",
      "fall, B-tim\n"
     ]
    }
   ],
   "source": [
    "# Try the output for the introduction example\n",
    "#sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
    "#sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
    "\n",
    "# New york times news:\n",
    "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "predictions = predict(sentence, model, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3920ea5-3a26-40ce-a610-d9be0d21f6d8",
   "metadata": {},
   "source": [
    "Peter B-per\n",
    "\n",
    "Navarro, I-per\n",
    "\n",
    "White B-org\n",
    "\n",
    "House I-org\n",
    "\n",
    "Sunday B-tim\n",
    "\n",
    "morning I-tim\n",
    "\n",
    "White B-org\n",
    "\n",
    "House I-org\n",
    "\n",
    "coronavirus B-tim\n",
    "\n",
    "fall, B-tim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd18f2-08bb-4634-8445-878c807e3540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
