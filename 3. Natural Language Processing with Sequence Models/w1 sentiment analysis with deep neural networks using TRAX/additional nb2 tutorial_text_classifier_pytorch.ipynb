{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG6vNyHEQoB1"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0KHwOZ0Q6jh",
        "outputId": "52ee101d-3092-4c43-d120-dc2b57b7243b",
        "collapsed": true
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0J_Sj_eRAO-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG0MHsxlRm0e"
      },
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/futurexskill/ml-model-deployment/main/Restaurant_Reviews.tsv.txt', delimiter= '\\t', quoting = 3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Qj4QUwy7RtEi",
        "outputId": "e18aff21-5790-4028-fb20-c7ad5b524df1"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Review  Liked\n",
              "0                           Wow... Loved this place.      1\n",
              "1                                 Crust is not good.      0\n",
              "2          Not tasty and the texture was just nasty.      0\n",
              "3  Stopped by during the late May bank holiday of...      1\n",
              "4  The selection on the menu was great and so wer...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-564d8a6e-dabc-47f7-8bfc-eeb33b49c955\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-564d8a6e-dabc-47f7-8bfc-eeb33b49c955')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-564d8a6e-dabc-47f7-8bfc-eeb33b49c955 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-564d8a6e-dabc-47f7-8bfc-eeb33b49c955');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6071966c-0155-4b2a-8cf4-2bfde758563f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6071966c-0155-4b2a-8cf4-2bfde758563f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6071966c-0155-4b2a-8cf4-2bfde758563f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 996,\n        \"samples\": [\n          \"They were excellent.\",\n          \"Your servers suck, wait, correction, our server Heimer sucked.\",\n          \"Will be back again!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Liked\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Lb1G3NSZ0L"
      },
      "source": [
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogW8b3DTGQ8"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIwfo-1GTKWD"
      },
      "source": [
        "ps = PorterStemmer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCxuGsgPTM3_",
        "outputId": "d9caf9ff-d1e8-4665-9d39-fbb7df64b9bc"
      },
      "source": [
        "dataset.info()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  1000 non-null   object\n",
            " 1   Liked   1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MC79MlzTRgY"
      },
      "source": [
        "corpus = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUseJMNPVUzH"
      },
      "source": [
        "import re\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH7ziNMCTYiy"
      },
      "source": [
        "for i in range(0, 1000):\n",
        "\n",
        "  customer_review = re.sub('[^a-zA-Z]', ' ',dataset['Review'][i])\n",
        "  customer_review = customer_review.lower()\n",
        "  customer_review = customer_review.split()\n",
        "  clean_review = [ps.stem(word) for word in customer_review if not word in set(stopwords.words('english'))]\n",
        "  clean_review = ' '.join(clean_review)\n",
        "  corpus.append(clean_review)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "Twuqh6fsVc6d",
        "outputId": "c44ffb15-2253-41af-b456-3db69bb1256b"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wow love place'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "DOr4rGoLVpAc",
        "outputId": "6dac0ed5-a4f7-45b4-e176-7497a5d4e5c1"
      },
      "source": [
        "corpus[6]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'honeslti tast fresh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "AmFOCByVVwpm",
        "outputId": "7ca22923-6db4-4cf3-aebb-53e40952ddf5"
      },
      "source": [
        "corpus[12]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cashier care ever say still end wayyy overpr'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGOH342dWDUU"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features = 1500, min_df = 3, max_df = 0.6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0uX9FMEWN-5"
      },
      "source": [
        "X = vectorizer.fit_transform(corpus).toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VJx99uhW3Uc",
        "outputId": "441ee86d-5397-47f1-8a07-335f2450ee2c"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eN5zp88W4D9",
        "outputId": "8bddffbf-10d5-4901-9c77-75beb44800eb"
      },
      "source": [
        "X[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.51611335, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.37891311, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.76814834, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cg_taoYW_gA"
      },
      "source": [
        "y = dataset.iloc[:, 1].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Acy_ksXRkz",
        "outputId": "def114f0-f8ac-41be-81cc-7d369563d693"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFwvHO6vXSQT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape # 467 features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHCU0narBqaK",
        "outputId": "9f47cc8b-1d43-48c8-a044-bf482fa4d148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 467)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOJMC0iDBxBZ",
        "outputId": "47d98f6e-490e-4f1c-e3ec-873417a65432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800,)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUHOUvsyS34"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_CqS6CVyM6B"
      },
      "source": [
        "Xtrain_ = torch.from_numpy(X_train).float()\n",
        "Xtest_ = torch.from_numpy(X_test).float()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTp5us_HXXEw"
      },
      "source": [
        "ytrain_ = torch.from_numpy(y_train)\n",
        "ytest_ = torch.from_numpy(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HcNA4eDXee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f1c6f8-f753-4a7e-b70d-20c2a88ca2fb"
      },
      "source": [
        "Xtrain_.shape, ytrain_.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([800, 467]), torch.Size([800]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZfiDEj3XnL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b32e2d-05cc-4e39-eb1f-1212bd8c93f0"
      },
      "source": [
        "Xtest_.shape, ytest_.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([200, 467]), torch.Size([200]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXVUjwR6XqI9"
      },
      "source": [
        "input_size   = 467 # yani jo max legth hogi tweet ki vo input rakha\n",
        "output_size  = 2\n",
        "hidden_size  = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZgwPR8tXsiE"
      },
      "source": [
        "class Net(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(Net, self).__init__()\n",
        "       self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
        "       self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "       self.fc3 = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "   def forward(self, X):\n",
        "       X = torch.relu((self.fc1(X)))\n",
        "       X = torch.relu((self.fc2(X)))\n",
        "       X = self.fc3(X)\n",
        "\n",
        "       return F.log_softmax(X,dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c933ZQOyXuQd"
      },
      "source": [
        "model = Net()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgnra037Xxl7"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn   = nn.NLLLoss()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCGzLzUHX7L_"
      },
      "source": [
        "epochs = 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYgpFHAbYDQf",
        "outputId": "16c6509e-4526-4778-b4e8-71706f63291b",
        "collapsed": true
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  Ypred = model(Xtrain_)\n",
        "\n",
        "  loss  = loss_fn(Ypred,  ytrain_)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Epoch',epoch, 'loss',loss.item())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss 0.6931865811347961\n",
            "Epoch 1 loss 0.6691359877586365\n",
            "Epoch 2 loss 0.5191556811332703\n",
            "Epoch 3 loss 0.3372277021408081\n",
            "Epoch 4 loss 0.20331089198589325\n",
            "Epoch 5 loss 0.12903030216693878\n",
            "Epoch 6 loss 0.09411798417568207\n",
            "Epoch 7 loss 0.08150523900985718\n",
            "Epoch 8 loss 0.06181521713733673\n",
            "Epoch 9 loss 0.04447095841169357\n",
            "Epoch 10 loss 0.041178103536367416\n",
            "Epoch 11 loss 0.04231702908873558\n",
            "Epoch 12 loss 0.034680724143981934\n",
            "Epoch 13 loss 0.03265057131648064\n",
            "Epoch 14 loss 0.034625254571437836\n",
            "Epoch 15 loss 0.031932879239320755\n",
            "Epoch 16 loss 0.029931241646409035\n",
            "Epoch 17 loss 0.031279075890779495\n",
            "Epoch 18 loss 0.030753836035728455\n",
            "Epoch 19 loss 0.031641747802495956\n",
            "Epoch 20 loss 0.03395778313279152\n",
            "Epoch 21 loss 0.029210280627012253\n",
            "Epoch 22 loss 0.03251252323389053\n",
            "Epoch 23 loss 0.038692425936460495\n",
            "Epoch 24 loss 0.0330270417034626\n",
            "Epoch 25 loss 0.033878814429044724\n",
            "Epoch 26 loss 0.03177274763584137\n",
            "Epoch 27 loss 0.03412388637661934\n",
            "Epoch 28 loss 0.03060893528163433\n",
            "Epoch 29 loss 0.029415350407361984\n",
            "Epoch 30 loss 0.032505299896001816\n",
            "Epoch 31 loss 0.030971992760896683\n",
            "Epoch 32 loss 0.029220249503850937\n",
            "Epoch 33 loss 0.030257491394877434\n",
            "Epoch 34 loss 0.030354097485542297\n",
            "Epoch 35 loss 0.029384449124336243\n",
            "Epoch 36 loss 0.03017345443367958\n",
            "Epoch 37 loss 0.02930656261742115\n",
            "Epoch 38 loss 0.028596293181180954\n",
            "Epoch 39 loss 0.030230753123760223\n",
            "Epoch 40 loss 0.030402526259422302\n",
            "Epoch 41 loss 0.028632164001464844\n",
            "Epoch 42 loss 0.029989302158355713\n",
            "Epoch 43 loss 0.029964955523610115\n",
            "Epoch 44 loss 0.028751881793141365\n",
            "Epoch 45 loss 0.029767530038952827\n",
            "Epoch 46 loss 0.02920367568731308\n",
            "Epoch 47 loss 0.029220685362815857\n",
            "Epoch 48 loss 0.029411910101771355\n",
            "Epoch 49 loss 0.028478434309363365\n",
            "Epoch 50 loss 0.02884536236524582\n",
            "Epoch 51 loss 0.02867194637656212\n",
            "Epoch 52 loss 0.02849404886364937\n",
            "Epoch 53 loss 0.028783263638615608\n",
            "Epoch 54 loss 0.028238985687494278\n",
            "Epoch 55 loss 0.02843397855758667\n",
            "Epoch 56 loss 0.028594953939318657\n",
            "Epoch 57 loss 0.028318244963884354\n",
            "Epoch 58 loss 0.028372226282954216\n",
            "Epoch 59 loss 0.02833007276058197\n",
            "Epoch 60 loss 0.028317466378211975\n",
            "Epoch 61 loss 0.028281498700380325\n",
            "Epoch 62 loss 0.02823643386363983\n",
            "Epoch 63 loss 0.02828633040189743\n",
            "Epoch 64 loss 0.028213124722242355\n",
            "Epoch 65 loss 0.028185367584228516\n",
            "Epoch 66 loss 0.028228353708982468\n",
            "Epoch 67 loss 0.028147084638476372\n",
            "Epoch 68 loss 0.028169289231300354\n",
            "Epoch 69 loss 0.028184836730360985\n",
            "Epoch 70 loss 0.028139982372522354\n",
            "Epoch 71 loss 0.028127752244472504\n",
            "Epoch 72 loss 0.02813280187547207\n",
            "Epoch 73 loss 0.02815467119216919\n",
            "Epoch 74 loss 0.028101855888962746\n",
            "Epoch 75 loss 0.028112508356571198\n",
            "Epoch 76 loss 0.028134284541010857\n",
            "Epoch 77 loss 0.02808360569179058\n",
            "Epoch 78 loss 0.028113694861531258\n",
            "Epoch 79 loss 0.028101973235607147\n",
            "Epoch 80 loss 0.02807673253118992\n",
            "Epoch 81 loss 0.02810925990343094\n",
            "Epoch 82 loss 0.028086664155125618\n",
            "Epoch 83 loss 0.028080789372324944\n",
            "Epoch 84 loss 0.028087107464671135\n",
            "Epoch 85 loss 0.02808516100049019\n",
            "Epoch 86 loss 0.028082439675927162\n",
            "Epoch 87 loss 0.02806995064020157\n",
            "Epoch 88 loss 0.028086040169000626\n",
            "Epoch 89 loss 0.028077110648155212\n",
            "Epoch 90 loss 0.028068549931049347\n",
            "Epoch 91 loss 0.028080428019165993\n",
            "Epoch 92 loss 0.028071334585547447\n",
            "Epoch 93 loss 0.028073078021407127\n",
            "Epoch 94 loss 0.028070218861103058\n",
            "Epoch 95 loss 0.028070013970136642\n",
            "Epoch 96 loss 0.028072016313672066\n",
            "Epoch 97 loss 0.028066884726285934\n",
            "Epoch 98 loss 0.028068650513887405\n",
            "Epoch 99 loss 0.028067199513316154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3cSNTmmYEg1"
      },
      "source": [
        "sample = [\"Good batting by England\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3qftBXlYO3E"
      },
      "source": [
        "sample = vectorizer.transform(sample).toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWRWMeafCjNX",
        "outputId": "2d38039e-cd8d-4554-f190-9a44f356fe4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 467)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpgyEuQCYTCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82984fa2-7812-4e5a-d13f-78c75def495c"
      },
      "source": [
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZNMlfQyYYBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf17146-1d33-4183-ac8d-3af44be7de5b"
      },
      "source": [
        "torch.from_numpy(sample).float()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJkJtzwKYaoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044b9505-74cf-4b2b-d626-d361d6f3b9d7"
      },
      "source": [
        "sentiment = model(torch.from_numpy(sample).float())\n",
        "sentiment\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.6069, -0.2238]], grad_fn=<LogSoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frLMoG4kYfWX"
      },
      "source": [
        "sample2 = [\"bad performance by India in the match\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcQ25_ne0K4w"
      },
      "source": [
        "sample2 = vectorizer.transform(sample2).toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIvI5szPYg0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4527193c-22d3-4371-a16b-5e12f4cae97c"
      },
      "source": [
        "sentiment2 = model(torch.from_numpy(sample2).float())\n",
        "sentiment2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.0000, -45.4522]], grad_fn=<LogSoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkZbp_CC0YTg",
        "outputId": "f1022c56-7ef1-4fae-a520-e421acc2b5b0",
        "collapsed": true
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('fc1.weight',\n",
              "              tensor([[ 0.0641,  0.0770, -0.0676,  ...,  0.0453,  0.0149,  0.0989],\n",
              "                      [-0.0714, -0.1119,  0.0938,  ...,  0.0486, -0.1365, -0.0973],\n",
              "                      [-0.1151,  0.0033, -0.1192,  ...,  0.1291, -0.0728, -0.0273],\n",
              "                      ...,\n",
              "                      [-0.0620, -0.0724,  0.0584,  ...,  0.0597,  0.0116, -0.1243],\n",
              "                      [ 0.0571,  0.0755, -0.0472,  ..., -0.0124, -0.0412,  0.1252],\n",
              "                      [-0.0435, -0.0056, -0.0929,  ...,  0.1140, -0.1361, -0.0281]])),\n",
              "             ('fc1.bias',\n",
              "              tensor([ 0.0667,  0.0201, -0.0303, -0.0059, -0.0123, -0.0210, -0.0512, -0.1127,\n",
              "                       0.0128,  0.0055, -0.0033,  0.0105, -0.0936, -0.0952,  0.0226,  0.0091,\n",
              "                       0.0238, -0.0126, -0.0869,  0.0545,  0.0159, -0.0171,  0.0028,  0.0516,\n",
              "                      -0.0031, -0.0218, -0.0145, -0.0877, -0.0915, -0.0239, -0.1144,  0.0162,\n",
              "                       0.0438,  0.0040,  0.0011, -0.0952, -0.0294,  0.0408, -0.0811, -0.1108,\n",
              "                      -0.0304, -0.1147,  0.0478, -0.0420,  0.0558, -0.0753, -0.0065,  0.0217,\n",
              "                       0.0116, -0.0213, -0.0195,  0.0086, -0.0207,  0.0094, -0.1279, -0.0140,\n",
              "                       0.0050, -0.0294, -0.0542, -0.0125, -0.0129, -0.0291,  0.0293,  0.0054,\n",
              "                      -0.1088,  0.0319,  0.0277, -0.0144,  0.0770, -0.0369, -0.0125, -0.0203,\n",
              "                       0.0281,  0.0151, -0.1017,  0.0221, -0.0182, -0.0390, -0.0127,  0.0051,\n",
              "                      -0.1048, -0.0139, -0.0515, -0.0101,  0.0010,  0.0227, -0.0320,  0.0443,\n",
              "                      -0.0110, -0.0303, -0.1004, -0.0589,  0.0184,  0.0277, -0.0034,  0.0559,\n",
              "                       0.0237,  0.0166,  0.0253, -0.0542,  0.0401, -0.0045,  0.0037,  0.0210,\n",
              "                      -0.1190, -0.0263,  0.0561,  0.0647, -0.0254, -0.0244, -0.1033, -0.0073,\n",
              "                       0.0534, -0.0099,  0.0083,  0.0323, -0.0639, -0.0499,  0.0096, -0.0799,\n",
              "                       0.0388,  0.0143,  0.0050,  0.0209,  0.0150,  0.0308, -0.0199, -0.0261,\n",
              "                      -0.0462, -0.0214, -0.0466, -0.0166, -0.0331,  0.0745,  0.0358, -0.0125,\n",
              "                       0.0288, -0.0198,  0.0705, -0.1072,  0.0477, -0.0411, -0.0901,  0.0284,\n",
              "                      -0.0174, -0.0113, -0.0188, -0.0398,  0.0193, -0.0937, -0.0021, -0.0133,\n",
              "                       0.0583,  0.0289,  0.0521,  0.0271,  0.0040,  0.0169, -0.1220, -0.0302,\n",
              "                      -0.1304, -0.0159,  0.0367, -0.0109, -0.1038, -0.0845,  0.0263, -0.0848,\n",
              "                      -0.0388,  0.0444,  0.0206, -0.0313,  0.0501,  0.0199, -0.0058,  0.0232,\n",
              "                      -0.0112, -0.0967,  0.0472,  0.0251,  0.0515,  0.0192,  0.0042, -0.0919,\n",
              "                      -0.0288,  0.0031, -0.0479,  0.0357, -0.0061, -0.0205, -0.0335, -0.0140,\n",
              "                       0.0080, -0.1270, -0.0175,  0.0404,  0.0021,  0.0181, -0.0244,  0.0283,\n",
              "                      -0.1029, -0.0779, -0.0549,  0.0086,  0.0212,  0.0287,  0.0110, -0.0043,\n",
              "                      -0.0150, -0.0114,  0.0346,  0.0085, -0.0261, -0.0306, -0.0507, -0.0349,\n",
              "                      -0.0513, -0.0068, -0.0846, -0.1062,  0.0088,  0.0428,  0.0460,  0.0423,\n",
              "                      -0.0120, -0.0540,  0.0119, -0.0920,  0.0431, -0.1225,  0.0035,  0.0439,\n",
              "                       0.0140, -0.0150, -0.0930,  0.0023,  0.0120,  0.0078, -0.0220, -0.1343,\n",
              "                      -0.0422,  0.0066,  0.0059,  0.0159, -0.0724, -0.0240, -0.0224,  0.0041,\n",
              "                       0.0375,  0.0108, -0.1065, -0.0648, -0.0270,  0.0496,  0.0270, -0.0478,\n",
              "                       0.0113,  0.0100, -0.0619, -0.0819, -0.0428, -0.0338, -0.0839, -0.0072,\n",
              "                       0.0183, -0.0174,  0.0161,  0.0455, -0.0628,  0.0190, -0.0378, -0.0805,\n",
              "                       0.0212,  0.0692,  0.0405, -0.0282, -0.0120,  0.0122,  0.0145, -0.0903,\n",
              "                      -0.0767, -0.1056,  0.0308, -0.0726, -0.0021,  0.0432, -0.0972,  0.0211,\n",
              "                       0.0270, -0.0733,  0.0029,  0.0470, -0.0983,  0.0037,  0.0958, -0.0330,\n",
              "                      -0.0227, -0.0270,  0.0101, -0.0153,  0.0125, -0.0171, -0.0764, -0.0147,\n",
              "                       0.0095,  0.0022, -0.0097,  0.0168,  0.0591, -0.1254,  0.0181, -0.0047,\n",
              "                      -0.0262, -0.0218, -0.0006,  0.0086,  0.0200,  0.0412, -0.0106,  0.0370,\n",
              "                       0.0184, -0.0502,  0.0222, -0.1031,  0.0432,  0.0329, -0.1074,  0.0156,\n",
              "                      -0.0459, -0.0161,  0.0151, -0.0586, -0.0205, -0.1150, -0.0223, -0.0201,\n",
              "                       0.0612, -0.0655,  0.0110, -0.0236, -0.0132,  0.0012,  0.0562,  0.0019,\n",
              "                      -0.0484, -0.0050, -0.0205,  0.0546, -0.1327, -0.0352, -0.0479, -0.1091,\n",
              "                       0.0310, -0.0914,  0.0206, -0.0489, -0.1113, -0.0107, -0.0933, -0.0168,\n",
              "                       0.0127,  0.0625, -0.0956, -0.0144, -0.0332, -0.0411, -0.0077,  0.0187,\n",
              "                       0.0051, -0.0047, -0.0117,  0.0472, -0.0030,  0.0319,  0.0394, -0.0873,\n",
              "                      -0.0897,  0.0724, -0.0250, -0.0195,  0.0200, -0.0307, -0.0119,  0.0556,\n",
              "                       0.0287, -0.0164, -0.0312,  0.0136,  0.0397, -0.0125, -0.1116,  0.0078,\n",
              "                       0.0102,  0.0064,  0.0058, -0.0687, -0.0463, -0.0444,  0.0656, -0.0180,\n",
              "                      -0.0203,  0.0426,  0.0156, -0.0041, -0.0189, -0.0167,  0.0126,  0.0169,\n",
              "                      -0.0403, -0.0254,  0.0036, -0.0278, -0.0824, -0.0047,  0.0311, -0.0123,\n",
              "                      -0.0188,  0.0090,  0.0576, -0.0370,  0.0246, -0.0411, -0.0738,  0.0191,\n",
              "                      -0.0391,  0.0101,  0.0461,  0.0120, -0.0636, -0.0369,  0.0292, -0.0647,\n",
              "                      -0.0176,  0.0815,  0.0165, -0.0209,  0.0471,  0.0070, -0.0972,  0.0034,\n",
              "                       0.0279,  0.0324,  0.0103,  0.0070,  0.0184, -0.0108, -0.0207, -0.0392,\n",
              "                      -0.0427,  0.0625,  0.0136, -0.0204,  0.0338,  0.0255,  0.0644,  0.0266,\n",
              "                       0.0319, -0.1076, -0.0565, -0.0295,  0.0340, -0.0181,  0.0109, -0.1273,\n",
              "                      -0.0173, -0.0285, -0.0069,  0.0040, -0.0300, -0.0214, -0.0306, -0.0559,\n",
              "                      -0.1100,  0.0041,  0.0424,  0.0500, -0.0187, -0.1129,  0.0451, -0.0572,\n",
              "                       0.0254,  0.0177,  0.0125, -0.0095,  0.0204, -0.0147, -0.0225, -0.0015,\n",
              "                      -0.0423, -0.0211, -0.0207,  0.0632, -0.1138, -0.0442,  0.0042, -0.0097,\n",
              "                       0.0031,  0.0435,  0.0374, -0.0509])),\n",
              "             ('fc2.weight',\n",
              "              tensor([[-0.0149,  0.0868,  0.1479,  ...,  0.0544, -0.0435, -0.1755],\n",
              "                      [ 0.0145,  0.0423,  0.0017,  ..., -0.0161, -0.0006, -0.0125],\n",
              "                      [ 0.0418, -0.0726, -0.0847,  ..., -0.0429,  0.0873,  0.1239],\n",
              "                      ...,\n",
              "                      [-0.0709, -0.0380,  0.0814,  ...,  0.0569, -0.0761,  0.0535],\n",
              "                      [ 0.0452, -0.0164, -0.0897,  ..., -0.0174,  0.1175,  0.1114],\n",
              "                      [ 0.0021,  0.0951,  0.1298,  ...,  0.1137, -0.0645, -0.1680]])),\n",
              "             ('fc2.bias',\n",
              "              tensor([ 0.0587, -0.0402,  0.0924, -0.0893, -0.0147, -0.0640,  0.0616, -0.0379,\n",
              "                       0.0075, -0.0899, -0.0768,  0.0458,  0.1044,  0.0172, -0.0315,  0.0545,\n",
              "                      -0.0248, -0.0311, -0.0313, -0.0678, -0.0619, -0.0469,  0.0548,  0.0460,\n",
              "                      -0.0884,  0.0192,  0.0160,  0.0414, -0.0902,  0.0769,  0.0334, -0.0645,\n",
              "                      -0.1020, -0.0167, -0.0691, -0.0510,  0.0786, -0.0639,  0.0634,  0.0427,\n",
              "                       0.0344, -0.0823, -0.0832, -0.0019,  0.0644, -0.0498,  0.0786,  0.0900,\n",
              "                      -0.0739, -0.0280,  0.0334, -0.0807,  0.0438, -0.0394,  0.0774,  0.0587,\n",
              "                       0.0563, -0.0389,  0.0140,  0.0895,  0.0024, -0.0207, -0.0835,  0.0582,\n",
              "                       0.0295,  0.0803,  0.0214,  0.0497, -0.0698,  0.0528, -0.0034,  0.0669,\n",
              "                      -0.0876,  0.0659, -0.0627,  0.0869, -0.0753,  0.0549,  0.0795,  0.0663,\n",
              "                       0.0918, -0.0767,  0.0939,  0.0293,  0.0446,  0.0239,  0.0740,  0.0377,\n",
              "                       0.0822, -0.0721, -0.0412,  0.0658, -0.0599,  0.0899,  0.0354, -0.0797,\n",
              "                       0.0879, -0.0698, -0.0766,  0.0566,  0.0489,  0.0181, -0.0501, -0.0648,\n",
              "                      -0.0014, -0.0257,  0.0598, -0.0842,  0.0768,  0.0636, -0.0581,  0.0632,\n",
              "                       0.0220,  0.0750,  0.0394,  0.0523, -0.0384,  0.0688,  0.0182, -0.0572,\n",
              "                      -0.0323,  0.0081, -0.0781, -0.0863,  0.0124,  0.0235,  0.0735, -0.0538,\n",
              "                      -0.0541, -0.0857, -0.0250,  0.0003, -0.0584, -0.0260, -0.0917,  0.0760,\n",
              "                      -0.1016, -0.0658, -0.0506,  0.0665,  0.0176, -0.0792,  0.0135, -0.0494,\n",
              "                      -0.0883,  0.0326,  0.1072,  0.0883,  0.0478,  0.0873,  0.0728, -0.0435,\n",
              "                      -0.0172,  0.0788,  0.0772, -0.0130,  0.0234,  0.0072,  0.0811,  0.1030,\n",
              "                       0.0944, -0.0801,  0.0661,  0.0492,  0.0449,  0.0611,  0.0468, -0.0698,\n",
              "                      -0.0254, -0.0448,  0.0963,  0.0830,  0.0868,  0.0757,  0.0320,  0.0455,\n",
              "                       0.0211,  0.0420,  0.0632,  0.0534, -0.0658, -0.0795,  0.0828, -0.0066,\n",
              "                      -0.0487, -0.0338, -0.0495,  0.0454,  0.0726,  0.0420,  0.0644,  0.0839,\n",
              "                      -0.0925,  0.0472,  0.0472,  0.0398, -0.0183,  0.0396,  0.0929, -0.0157,\n",
              "                      -0.0556, -0.0401,  0.0678, -0.0546,  0.0882, -0.0792,  0.0731, -0.0195,\n",
              "                       0.0262, -0.0432, -0.0569, -0.0429, -0.0279, -0.0846, -0.0737, -0.0668,\n",
              "                       0.0227, -0.0491,  0.0144, -0.0526, -0.0236, -0.0320,  0.0707,  0.0562,\n",
              "                      -0.0785,  0.0186, -0.0404, -0.0733,  0.0531,  0.0689, -0.0794,  0.0341,\n",
              "                      -0.0216,  0.0014,  0.0220,  0.0557,  0.0949, -0.0220, -0.0937,  0.0747,\n",
              "                      -0.0920, -0.0622, -0.0692,  0.0754,  0.0764,  0.0756,  0.0615, -0.0521,\n",
              "                      -0.0757,  0.0477,  0.0215,  0.1001, -0.0536,  0.0498, -0.0217,  0.0531,\n",
              "                      -0.0346, -0.0813, -0.0269, -0.0740,  0.0306, -0.0319,  0.0397,  0.0230,\n",
              "                      -0.0077,  0.0469,  0.0664,  0.0667, -0.0689, -0.0832, -0.0157,  0.0526,\n",
              "                      -0.0827, -0.0969,  0.0529,  0.0438,  0.0647,  0.0819,  0.0944, -0.0852,\n",
              "                      -0.0845,  0.0611, -0.0313,  0.0143, -0.0732,  0.0744,  0.0712, -0.0482,\n",
              "                      -0.0207, -0.0655,  0.0015, -0.0180,  0.0998, -0.0695, -0.0802, -0.0213,\n",
              "                       0.1055,  0.0893, -0.0702,  0.0849,  0.0704,  0.0487,  0.0315,  0.0522,\n",
              "                       0.0195, -0.0653,  0.0213,  0.0667, -0.0353,  0.0770,  0.0755,  0.0864,\n",
              "                      -0.0584,  0.0664,  0.0747, -0.0991, -0.0718,  0.0713,  0.0737,  0.0756,\n",
              "                       0.0831,  0.0646,  0.0887,  0.1140, -0.0845, -0.0210, -0.0586, -0.0340,\n",
              "                       0.0803,  0.0728,  0.0762,  0.0485, -0.0654, -0.0792, -0.0403,  0.0810,\n",
              "                      -0.0481, -0.0397, -0.0666, -0.0089, -0.0602, -0.0630, -0.0852, -0.0087,\n",
              "                       0.0405,  0.0635, -0.0191, -0.0141,  0.0644,  0.0719,  0.0458, -0.0822,\n",
              "                       0.0596, -0.0801,  0.0566,  0.0734, -0.0253, -0.0739, -0.0550,  0.0632,\n",
              "                       0.0559,  0.0641, -0.0249,  0.0893, -0.0191, -0.0745,  0.0925,  0.0288,\n",
              "                       0.0667,  0.0769, -0.0905, -0.0674,  0.0395,  0.0795, -0.0534,  0.0582,\n",
              "                      -0.0670,  0.0489,  0.0634,  0.0817,  0.0324, -0.0481,  0.0883, -0.0432,\n",
              "                       0.0949,  0.0032,  0.0331, -0.0266,  0.0176,  0.0577,  0.0075, -0.0596,\n",
              "                       0.0627, -0.0511,  0.0413,  0.0327, -0.0787,  0.0797,  0.0760,  0.0661,\n",
              "                      -0.0378, -0.0693, -0.0314, -0.0695, -0.0056, -0.1004, -0.0764,  0.0919,\n",
              "                      -0.0768,  0.1128,  0.0596,  0.0608,  0.0059,  0.0181,  0.0763,  0.0681,\n",
              "                      -0.0768,  0.0519, -0.0682,  0.0674,  0.0576, -0.0627, -0.0879,  0.0835,\n",
              "                       0.0531,  0.0743, -0.0221,  0.0651, -0.0564, -0.0062,  0.0504,  0.0548,\n",
              "                      -0.0355,  0.0265,  0.0557, -0.0287,  0.0746, -0.0796, -0.0034,  0.0796,\n",
              "                      -0.0126,  0.0735,  0.0888, -0.0548, -0.0253, -0.0753,  0.0431, -0.0960,\n",
              "                       0.0837, -0.0837,  0.0890,  0.0310, -0.0395,  0.0939, -0.0875,  0.0730,\n",
              "                      -0.0354,  0.0594,  0.0513, -0.0355,  0.0219, -0.0434,  0.0327, -0.0551,\n",
              "                      -0.0546, -0.0324,  0.0182,  0.0130, -0.1032,  0.0560, -0.0777,  0.0084,\n",
              "                       0.0785, -0.0786, -0.0172,  0.0328,  0.0422, -0.0606, -0.0133,  0.0280,\n",
              "                      -0.0520, -0.0693, -0.0689, -0.0556, -0.0020, -0.0625, -0.0316, -0.0672,\n",
              "                       0.0343,  0.0501,  0.0581,  0.0545,  0.0074, -0.0621,  0.0572,  0.0548,\n",
              "                       0.0510, -0.0780,  0.0853,  0.0858])),\n",
              "             ('fc3.weight',\n",
              "              tensor([[-7.7491e-02,  1.6623e-02,  1.2861e-01, -4.7464e-02,  5.7361e-02,\n",
              "                        4.5142e-02, -8.7927e-02,  4.9801e-02,  3.7653e-02, -2.1277e-02,\n",
              "                       -6.1708e-02, -9.9704e-02,  9.8450e-02, -1.0046e-01, -4.2193e-02,\n",
              "                       -1.1661e-01,  1.4999e-02, -1.0931e-01,  4.2011e-03, -8.7057e-02,\n",
              "                        1.8243e-02, -1.8817e-02,  1.2284e-01,  1.1893e-01, -1.0305e-01,\n",
              "                       -8.3650e-02, -7.3670e-02, -1.1321e-01, -4.4681e-02,  1.2224e-01,\n",
              "                        1.0821e-01, -6.3124e-02, -9.8356e-02,  2.6183e-02,  5.5941e-02,\n",
              "                       -3.2355e-02, -7.7694e-02,  2.9795e-02,  1.1715e-01,  7.4452e-02,\n",
              "                        1.0121e-01,  8.1132e-03, -4.8811e-02,  1.5357e-02, -1.0441e-01,\n",
              "                       -3.1699e-02, -1.0994e-01, -9.0077e-02, -4.4898e-02,  3.8972e-02,\n",
              "                        1.0793e-01, -2.0535e-02,  6.2515e-02,  4.9350e-02, -1.2594e-01,\n",
              "                        8.0433e-02, -1.0932e-01,  3.5226e-02,  6.8161e-02, -1.0543e-01,\n",
              "                       -6.8052e-02,  3.5213e-02,  2.2150e-02, -9.0845e-03, -6.6848e-02,\n",
              "                       -1.2629e-01, -8.9305e-02,  7.1979e-02, -9.6328e-02, -8.7653e-02,\n",
              "                       -8.3019e-02,  1.1336e-01, -3.4020e-02, -1.1581e-01,  5.9763e-02,\n",
              "                        5.5287e-02, -7.2846e-02, -1.0463e-01, -9.0182e-02,  9.4682e-02,\n",
              "                        1.2178e-01,  2.2167e-02,  8.7900e-02, -7.7376e-02, -9.4827e-02,\n",
              "                        8.1814e-02,  1.2562e-01,  4.8464e-02, -1.3894e-01,  2.5620e-02,\n",
              "                       -3.2029e-03, -1.0242e-01,  4.8107e-02, -1.1724e-01, -6.4641e-02,\n",
              "                       -5.4618e-02, -1.0512e-01, -9.8171e-02, -9.1872e-03,  7.5984e-02,\n",
              "                        1.1967e-01,  7.3485e-02, -7.8421e-02,  4.1634e-02, -9.6415e-03,\n",
              "                       -8.7155e-02,  8.9565e-02,  6.8383e-02, -1.2114e-01, -5.1776e-02,\n",
              "                        3.4339e-02, -8.3797e-02, -1.2269e-01, -4.2906e-02, -8.9903e-02,\n",
              "                       -7.3727e-02, -5.6622e-02, -1.3987e-01,  5.4162e-02,  9.1229e-02,\n",
              "                        2.2907e-02,  8.6034e-02,  4.4527e-02,  5.9565e-03,  6.0446e-02,\n",
              "                       -2.0927e-02, -1.2526e-01, -5.5549e-02,  3.9274e-02,  1.6868e-02,\n",
              "                        8.1837e-03, -7.5080e-02, -6.0855e-02, -3.7148e-02,  6.0679e-02,\n",
              "                       -7.4269e-02, -6.9689e-02, -2.0540e-02, -3.6635e-03,  8.2810e-02,\n",
              "                       -3.6787e-02,  2.3255e-02,  7.7394e-03,  1.7672e-02, -2.4844e-02,\n",
              "                       -2.6186e-02,  9.1091e-02, -1.1674e-01,  8.5123e-02, -1.2498e-01,\n",
              "                       -1.1383e-01,  3.8629e-02,  3.0711e-02, -9.2305e-02, -1.0090e-01,\n",
              "                        3.3586e-02, -7.5612e-02, -5.5130e-03, -1.0428e-01,  1.1144e-01,\n",
              "                        1.1247e-01, -2.7664e-02,  1.2194e-01, -1.3428e-01,  6.9496e-02,\n",
              "                        6.0802e-02, -1.7136e-02,  3.8267e-02, -4.7887e-02,  4.8739e-03,\n",
              "                        8.6449e-02,  1.2440e-01, -9.0078e-02, -9.2438e-02, -5.7697e-02,\n",
              "                        8.8261e-02, -1.3165e-01,  1.1353e-01,  9.5471e-02,  7.9325e-02,\n",
              "                       -8.2180e-02,  3.2960e-02, -1.1903e-01, -3.2568e-02,  4.3161e-02,\n",
              "                       -6.6471e-02, -1.3142e-02, -7.3012e-02,  7.9406e-02, -1.1679e-01,\n",
              "                       -3.2473e-03,  1.1860e-01,  3.2204e-02,  5.0189e-02,  9.0767e-02,\n",
              "                        7.1445e-02, -1.1514e-01,  1.1824e-01,  1.3234e-01,  2.8853e-02,\n",
              "                       -2.8053e-02, -1.2979e-01, -9.5577e-02, -2.0136e-02,  1.2613e-01,\n",
              "                        5.8369e-02,  8.5489e-02, -1.1151e-01, -7.9789e-02, -6.3285e-02,\n",
              "                        6.4833e-03,  3.1331e-02,  3.7045e-02, -2.6292e-02, -6.2733e-02,\n",
              "                       -4.4151e-02,  7.8859e-02,  3.4932e-02,  1.0988e-01,  3.7984e-02,\n",
              "                       -5.2825e-02, -1.8227e-02,  9.5317e-02, -1.2122e-01, -9.2859e-03,\n",
              "                        1.0964e-01, -1.7794e-02,  2.4183e-02, -9.0846e-02, -1.0125e-01,\n",
              "                        2.7249e-02,  8.6224e-02, -7.8729e-02, -1.1020e-01, -1.1025e-01,\n",
              "                       -1.3894e-01,  1.1254e-01, -3.8736e-02, -6.1880e-02, -1.1725e-01,\n",
              "                        1.8845e-02, -3.8250e-02, -6.4854e-02,  9.0710e-02, -5.1054e-02,\n",
              "                       -8.6384e-02,  4.2870e-02,  4.3549e-02, -2.2264e-02,  1.2272e-01,\n",
              "                       -4.5811e-03,  1.2635e-01, -4.1028e-02,  1.0344e-01, -1.0183e-01,\n",
              "                       -7.2674e-02,  6.2194e-03,  1.5654e-02, -2.8094e-02,  5.4001e-02,\n",
              "                        6.0320e-02, -2.1108e-02, -6.6827e-02, -6.6678e-02, -2.4639e-02,\n",
              "                       -9.1301e-02,  8.4816e-02, -1.0268e-01, -2.1707e-03,  1.1762e-02,\n",
              "                       -7.6761e-02,  8.7430e-02, -5.7885e-02, -1.6028e-02,  9.7801e-02,\n",
              "                        9.0604e-02, -1.1633e-01, -1.3211e-01, -1.0527e-01, -3.6835e-02,\n",
              "                        3.2285e-02,  5.9053e-02, -4.6043e-02, -4.4540e-02, -5.6182e-02,\n",
              "                       -1.1164e-01,  9.8176e-02, -6.4938e-02,  8.5213e-03, -1.0501e-01,\n",
              "                       -1.0115e-01,  1.3539e-02,  5.8955e-02, -5.4887e-02,  4.6705e-02,\n",
              "                       -2.3643e-03,  1.0030e-01, -1.3777e-01, -6.6863e-02, -1.0072e-01,\n",
              "                        1.0568e-01, -1.2318e-01, -6.8594e-02,  8.8927e-02, -3.0537e-04,\n",
              "                        6.4797e-02,  7.0741e-02,  4.4252e-02, -7.0932e-02, -1.1006e-01,\n",
              "                       -7.4745e-02, -7.5745e-02, -2.3800e-02,  8.3775e-02,  4.8945e-02,\n",
              "                       -6.6091e-02, -1.8310e-02,  1.1179e-01,  9.5919e-02, -8.9171e-02,\n",
              "                        1.1697e-01, -1.1245e-01, -1.2186e-01,  1.3781e-01, -5.3142e-02,\n",
              "                       -8.2954e-02,  3.5440e-02, -1.0156e-01, -3.9376e-02,  6.4849e-02,\n",
              "                        1.1341e-01,  5.7417e-02,  2.0178e-02, -4.7917e-03, -8.1122e-03,\n",
              "                       -1.2213e-01, -2.0182e-02, -2.2528e-03, -6.9562e-02, -9.7661e-02,\n",
              "                       -2.7737e-02,  4.0372e-02,  7.9380e-02, -4.2585e-02, -8.7406e-02,\n",
              "                       -1.1057e-01, -1.8807e-02, -5.9907e-02,  9.4427e-02,  1.0584e-01,\n",
              "                       -1.2899e-01, -3.7934e-02, -1.1075e-01, -5.9498e-02,  1.1469e-01,\n",
              "                       -9.3919e-02, -4.1923e-02, -1.1598e-02,  2.7600e-02, -8.2394e-02,\n",
              "                        8.0257e-02, -1.1742e-01,  3.7504e-02,  1.2496e-01, -3.0008e-02,\n",
              "                        3.5382e-02,  1.1210e-01, -5.8037e-02, -1.0602e-01, -1.2895e-01,\n",
              "                       -1.2415e-02,  1.8823e-02,  1.1526e-01, -1.1463e-01, -5.1926e-02,\n",
              "                       -9.9623e-02,  5.8801e-02, -1.1366e-01,  1.1121e-01, -1.2658e-01,\n",
              "                       -9.5050e-02,  2.4378e-02,  1.0165e-01, -2.3343e-02,  1.2727e-01,\n",
              "                        4.0864e-02, -5.4371e-02, -3.8804e-02,  3.6109e-02,  4.5889e-02,\n",
              "                       -7.3301e-02, -8.9226e-02,  7.7741e-02, -2.4795e-02, -1.1528e-01,\n",
              "                        8.1239e-02,  3.7089e-02, -5.9785e-02,  8.8491e-02, -1.2700e-01,\n",
              "                        1.4648e-02, -4.4938e-02, -3.9345e-02, -1.5804e-02,  2.6523e-02,\n",
              "                        3.8622e-02, -1.6344e-03,  6.1166e-02,  7.2082e-02,  9.3102e-02,\n",
              "                       -9.9488e-02, -3.9654e-02, -9.4339e-02,  8.2865e-02,  6.1026e-02,\n",
              "                        6.5843e-02, -1.0065e-01,  5.9976e-02,  1.8498e-02, -1.3080e-01,\n",
              "                       -9.9436e-02, -4.8516e-02, -7.3267e-02,  8.6944e-02, -2.8966e-02,\n",
              "                        7.8306e-02, -2.4470e-02, -1.1359e-01, -5.1212e-02, -2.6990e-03,\n",
              "                       -1.0525e-01, -7.1981e-02,  4.9441e-02,  1.1122e-01,  1.0714e-01,\n",
              "                       -1.4807e-02,  1.3528e-01,  2.4927e-02, -1.2204e-01, -5.3166e-02,\n",
              "                        1.8809e-02, -1.2778e-01,  1.2611e-01,  1.8628e-02,  3.3021e-03,\n",
              "                       -4.1351e-02,  8.3844e-02, -4.4482e-02, -5.4977e-02, -2.4114e-02,\n",
              "                        1.1467e-01,  1.1758e-02,  3.6356e-02,  1.0972e-01,  1.6368e-02,\n",
              "                       -1.0043e-01,  3.1877e-02,  5.0770e-02,  8.3929e-02,  4.0717e-02,\n",
              "                        5.8964e-02,  7.2217e-02, -4.0756e-02, -3.7579e-02, -4.0568e-02,\n",
              "                        1.8555e-02,  1.2367e-02,  9.3839e-02, -2.2027e-02, -1.0595e-01,\n",
              "                        1.9494e-02, -1.0355e-01, -8.3282e-02,  4.2339e-02, -9.0322e-02,\n",
              "                        6.9606e-02, -4.9676e-02, -7.1317e-02, -1.2551e-01, -1.7250e-02,\n",
              "                       -2.2058e-02,  8.1768e-02,  2.2383e-02,  3.0675e-02,  3.0493e-02,\n",
              "                       -1.9272e-02,  1.2532e-02, -5.5369e-02,  1.0655e-01,  1.2410e-01,\n",
              "                        9.0837e-02,  5.7503e-02,  5.9112e-02,  4.2433e-02, -6.6323e-02,\n",
              "                        1.1734e-01,  1.0068e-01,  1.8482e-02,  1.2568e-01, -8.5105e-02],\n",
              "                      [ 8.0836e-02, -4.0818e-02, -7.1274e-02,  4.9197e-02, -3.8176e-02,\n",
              "                       -4.6765e-02,  1.0470e-01, -5.4603e-02, -7.6583e-02,  3.6017e-02,\n",
              "                        5.3809e-02,  7.4471e-02, -7.4944e-02,  7.8239e-02, -2.4261e-02,\n",
              "                        1.2076e-01, -9.4125e-03,  5.2600e-02, -6.4779e-02,  1.0980e-01,\n",
              "                       -4.3970e-02,  2.4411e-02, -6.8198e-02, -7.4821e-02,  4.1092e-02,\n",
              "                        1.2345e-01,  9.8648e-02,  8.5634e-02,  4.5351e-02, -6.8129e-02,\n",
              "                       -8.9052e-02,  5.5001e-02,  6.5657e-02, -7.0620e-02, -4.7061e-02,\n",
              "                        3.5741e-02,  1.0705e-01, -2.6622e-02, -9.2907e-02, -7.6669e-02,\n",
              "                       -1.2037e-01, -5.8847e-02,  4.8039e-02, -8.1710e-02,  1.3222e-01,\n",
              "                        5.9444e-02,  1.2515e-01,  1.3653e-01,  2.9174e-02, -5.3592e-02,\n",
              "                       -1.1441e-01,  9.9045e-02, -8.8733e-02, -1.9683e-02,  1.3633e-01,\n",
              "                       -8.3257e-02,  1.2380e-01, -3.1965e-02, -3.5520e-02,  8.6607e-02,\n",
              "                        8.1312e-02,  1.5504e-03, -4.9552e-02, -1.5758e-02,  9.5178e-02,\n",
              "                        1.3477e-01,  5.7725e-02, -1.1884e-01,  2.2702e-02,  9.1895e-02,\n",
              "                        7.7158e-02, -1.1391e-01,  6.6757e-02,  1.0713e-01, -2.4892e-02,\n",
              "                       -1.1319e-01,  9.2984e-03,  8.5264e-02,  1.2467e-01, -1.0692e-01,\n",
              "                       -9.7520e-02, -7.9342e-02, -1.0334e-01,  1.0750e-01,  1.0388e-01,\n",
              "                       -1.0291e-01, -8.0171e-02, -1.0199e-01,  1.3546e-01, -7.8705e-02,\n",
              "                        1.1814e-02,  9.8002e-02, -5.5081e-02,  9.2950e-02,  1.1309e-01,\n",
              "                        4.4983e-02,  8.9582e-02,  1.9230e-02,  5.0673e-02, -1.0028e-01,\n",
              "                       -9.2387e-02, -1.0342e-01,  2.5505e-02, -1.0316e-02, -2.7303e-02,\n",
              "                        1.3318e-02, -6.6472e-02, -2.8774e-02,  9.9425e-02,  1.2283e-01,\n",
              "                       -3.9934e-02,  1.0512e-01,  9.8475e-02,  1.0530e-01,  8.2584e-02,\n",
              "                        5.0139e-02,  4.0218e-02,  5.7494e-02, -1.7941e-02, -2.8461e-02,\n",
              "                       -3.1964e-02, -3.7336e-02, -6.1858e-02, -3.9058e-02, -6.2096e-02,\n",
              "                        9.6562e-02,  1.1940e-01,  4.8078e-02, -8.1678e-04, -3.5486e-02,\n",
              "                       -2.6083e-02,  9.6013e-02,  8.1238e-02, -1.2941e-03, -3.6581e-02,\n",
              "                        1.0234e-01,  2.1480e-02,  2.7846e-02,  1.7394e-03, -9.8791e-02,\n",
              "                        9.5981e-02, -5.6411e-02, -8.0636e-03,  2.9629e-02,  6.3582e-02,\n",
              "                        8.0210e-02, -1.0285e-01,  9.5083e-02, -7.4359e-02,  7.7708e-02,\n",
              "                        1.1286e-01,  3.8465e-02,  3.1103e-02,  1.2615e-01,  9.8365e-02,\n",
              "                       -5.1842e-02,  4.8533e-02,  2.9306e-02,  1.0590e-01, -9.8475e-02,\n",
              "                       -8.7567e-02,  7.4586e-02, -9.6520e-02,  9.1686e-02, -1.0021e-01,\n",
              "                       -9.7411e-02, -5.9134e-02, -4.2286e-02,  6.7234e-02,  3.9944e-02,\n",
              "                       -9.8334e-02, -7.2309e-02,  1.0085e-01,  7.6993e-02,  4.0602e-02,\n",
              "                       -4.2760e-02,  7.7644e-02, -6.4077e-02, -9.4808e-02, -7.0432e-02,\n",
              "                        9.1338e-02,  3.8038e-02,  1.2466e-01, -1.6116e-04, -2.5116e-02,\n",
              "                        1.1835e-01, -9.8281e-03,  1.1214e-01, -8.6065e-02,  7.2810e-02,\n",
              "                        5.0790e-02, -9.1750e-02, -7.8429e-02, -1.0060e-01, -9.4393e-02,\n",
              "                       -8.6805e-02,  1.2463e-01, -1.0201e-01, -9.7401e-02, -5.7563e-02,\n",
              "                        2.3613e-02,  5.9039e-02,  7.9642e-02,  6.1789e-02, -6.6321e-02,\n",
              "                       -1.3846e-02, -8.8187e-02,  8.7585e-02,  5.0993e-02,  4.4069e-02,\n",
              "                       -6.4070e-03, -6.0729e-03, -5.1039e-02,  4.0192e-02,  5.6554e-02,\n",
              "                        4.2623e-02, -1.0007e-01, -2.4834e-02, -9.0507e-02, -1.8732e-02,\n",
              "                        8.6271e-02,  2.3830e-02, -1.0470e-01,  6.3683e-02,  3.0888e-02,\n",
              "                       -7.5859e-02,  5.5658e-02, -4.0693e-02,  1.1877e-01,  8.4943e-02,\n",
              "                       -6.9752e-02, -1.1159e-01,  5.3595e-02,  1.0067e-01,  7.5619e-02,\n",
              "                        8.6609e-02, -1.0041e-01,  3.4080e-02,  9.1417e-03,  1.1027e-01,\n",
              "                        2.3890e-03,  6.6644e-02,  3.2969e-02, -1.1442e-01,  1.0686e-01,\n",
              "                        1.3523e-01, -7.1395e-02, -6.4322e-02,  3.5204e-02, -1.2843e-01,\n",
              "                        7.9327e-02, -1.2773e-01,  8.7744e-02, -4.3191e-02,  5.2040e-02,\n",
              "                        1.2153e-01,  1.9815e-02, -8.1078e-02, -3.6701e-02,  7.6919e-03,\n",
              "                       -1.0764e-01,  9.2077e-03,  1.0721e-01,  7.9115e-02, -2.8117e-02,\n",
              "                        1.1229e-01, -7.8396e-02,  1.0222e-01,  1.6250e-02, -2.8290e-02,\n",
              "                        1.0072e-01, -1.0321e-01,  5.3674e-02,  5.6728e-03, -1.3590e-01,\n",
              "                       -1.2165e-01,  8.5837e-02,  7.5626e-02,  9.6800e-02,  3.4324e-02,\n",
              "                       -4.7123e-02, -8.9128e-02,  2.1322e-02,  1.1384e-01,  5.9033e-02,\n",
              "                        7.1869e-02, -1.0948e-01,  8.8209e-02, -2.6880e-02,  1.3055e-01,\n",
              "                        6.2891e-02,  3.8676e-02, -1.1158e-01,  1.2972e-03, -5.9871e-02,\n",
              "                        6.7587e-04, -1.0393e-01,  8.5870e-02,  1.7003e-02,  1.3493e-01,\n",
              "                       -7.8894e-02,  1.2382e-01,  8.4452e-02, -7.6213e-02, -1.6515e-02,\n",
              "                        5.0818e-03, -6.8539e-02, -1.1718e-01,  6.6084e-02,  1.2044e-01,\n",
              "                        1.0477e-01,  1.1971e-01,  4.8120e-02, -7.6700e-02, -1.2802e-01,\n",
              "                        9.1723e-02,  1.9671e-02, -1.2199e-01, -8.5401e-02,  1.0954e-01,\n",
              "                       -1.2273e-01,  1.1197e-01,  1.0485e-01, -1.3269e-01, -2.8950e-03,\n",
              "                        7.8035e-02, -1.6081e-02,  8.6354e-02,  1.0534e-01, -1.1152e-01,\n",
              "                       -1.2679e-01, -7.0157e-02, -2.8568e-02, -3.5222e-02,  1.2064e-02,\n",
              "                        5.2930e-02,  1.8107e-03,  2.9934e-02,  4.1478e-02,  1.0084e-01,\n",
              "                        4.7018e-02, -4.0841e-02, -8.5777e-02,  4.8603e-02,  1.0729e-01,\n",
              "                        9.0309e-02,  3.3478e-02,  9.8161e-02, -6.6284e-02, -1.0399e-01,\n",
              "                        1.1602e-01,  4.5814e-02,  5.7793e-02,  3.9882e-02, -1.0064e-01,\n",
              "                        7.7064e-02,  6.2250e-02,  3.7431e-02, -3.3092e-02,  1.2382e-01,\n",
              "                       -1.3423e-01,  9.3039e-02,  2.0449e-02, -9.7359e-02,  1.7077e-02,\n",
              "                       -4.3302e-02, -7.8768e-02,  8.9977e-02,  7.4271e-02,  5.9584e-02,\n",
              "                        3.5806e-02, -5.1166e-02, -1.1299e-01,  8.8726e-02,  2.3653e-02,\n",
              "                        9.8187e-02, -3.1204e-02,  6.4776e-02, -9.2118e-02,  6.2619e-02,\n",
              "                        7.8265e-02, -8.5689e-02, -6.8535e-02, -2.2124e-05, -9.7503e-02,\n",
              "                       -1.2639e-02,  1.1663e-01, -2.2111e-02, -9.0414e-02, -1.0359e-01,\n",
              "                        6.2582e-02,  6.8854e-02, -8.3721e-02, -5.4618e-03,  8.7393e-02,\n",
              "                       -1.1035e-01, -1.6222e-04,  1.0539e-01, -1.2248e-01,  8.1152e-02,\n",
              "                        3.8206e-02,  7.2733e-02,  7.4435e-02,  8.0640e-02,  1.5943e-03,\n",
              "                       -5.0923e-02, -6.8609e-02, -1.3139e-01, -3.7338e-02, -1.2491e-01,\n",
              "                        1.1475e-01,  1.0172e-01,  9.3841e-02, -1.0377e-01, -1.0878e-01,\n",
              "                       -9.3834e-02,  1.7022e-02, -9.9430e-02, -1.8800e-02,  6.2820e-02,\n",
              "                        5.1441e-02,  2.5645e-02,  3.7347e-02, -7.6051e-02,  3.5985e-02,\n",
              "                       -1.1120e-01, -5.6150e-02,  7.3480e-02,  4.8202e-02, -3.0887e-02,\n",
              "                        3.5736e-02,  1.3371e-01, -3.6714e-02, -6.0674e-02, -8.6774e-02,\n",
              "                       -4.2332e-02, -1.0363e-01, -5.5824e-02,  8.2161e-02,  1.3414e-01,\n",
              "                       -3.6765e-02,  1.2453e-01, -4.7406e-02,  6.3271e-03, -4.2390e-02,\n",
              "                        1.6483e-02, -1.0360e-01,  5.8022e-02,  1.2148e-01,  9.6129e-02,\n",
              "                       -7.1627e-02, -3.7278e-02, -4.7978e-02, -7.8704e-02, -6.1163e-02,\n",
              "                        1.0148e-01, -3.3176e-02, -1.0221e-01, -1.1662e-01, -3.4008e-02,\n",
              "                       -1.1660e-01, -1.8776e-02,  3.8592e-02, -4.0081e-02,  2.7396e-02,\n",
              "                       -1.0589e-02, -4.0105e-02, -7.6927e-02,  4.0928e-02,  1.4097e-01,\n",
              "                       -3.5795e-02,  7.6540e-02,  1.0653e-01, -2.8121e-02,  3.9511e-02,\n",
              "                       -1.3081e-01,  3.6881e-02,  5.4015e-02,  1.0189e-01,  2.1846e-02,\n",
              "                        2.5714e-02, -4.2996e-02, -7.2302e-02, -5.7542e-02, -4.6269e-02,\n",
              "                        6.6001e-02,  4.0471e-02,  9.5672e-03, -2.8275e-02, -7.2070e-02,\n",
              "                       -9.8625e-02, -7.1757e-02, -3.3285e-02,  6.6752e-03,  7.9989e-02,\n",
              "                       -6.6813e-02, -1.2747e-01, -5.9731e-02, -5.3889e-02,  1.0433e-01]])),\n",
              "             ('fc3.bias', tensor([ 0.0213, -0.0468]))])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfseKPHg0hOG"
      },
      "source": [
        "torch.save(model.state_dict(),'text_classifier_pytorch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ4iBouH0pkb",
        "outputId": "09ad583a-3241-49fa-f25e-e2810ac5f120"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  text_classifier_pytorch  tfidfmodel.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwKFztcGHdRN"
      },
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "leF7ttuWIBGh",
        "outputId": "7bd2120b-811b-492d-84e5-2672be0fbdbf"
      },
      "source": [
        "files.download('text_classifier_pytorch')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e8e6e9f8-d1e1-4c0e-b11e-1dbac06be6ed\", \"text_classifier_pytorch\", 1944129)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5i97IE-35t3"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMvo1EXgILg1"
      },
      "source": [
        "with open('tfidfmodel.pickle','wb') as file:\n",
        "    pickle.dump(vectorizer,file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iCoZZiLY32Cf",
        "outputId": "7ff4f9c3-dd88-4ca9-f2b0-04d89d2c1ef1"
      },
      "source": [
        "files.download('tfidfmodel.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a8c8eb19-8436-4c10-849b-53ad20da306a\", \"tfidfmodel.pickle\", 49660)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfBgQDtc39yV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}