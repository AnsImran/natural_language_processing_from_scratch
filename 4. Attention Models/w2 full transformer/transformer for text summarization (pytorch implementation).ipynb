{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8e39624c-448c-4eee-884f-94971dca02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp c4w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a11806de-a7dd-4e07-a487-6821f6968e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import log_softmax\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import utils\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd01445-2d0c-40b3-9f36-bcc6b990a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f85949-3545-4134-a288-cb46e74fc933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db139ff5-9b86-46ea-925c-99dda245f848",
   "metadata": {},
   "source": [
    "## Import the Dataset\n",
    "You have the dataset saved in a .json file, which you can easily open with pandas. The loading function has already been taken care of in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "417191b5-7d4e-4acf-94fa-3e4be5d3b79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Lucas: Hey! How was your day?\n",
      "Demi: Hey there! \n",
      "Demi: It was pretty fine, actually, thank you!\n",
      "Demi: I just got promoted! :D\n",
      "Lucas: Whoa! Great news!\n",
      "Lucas: Congratulations!\n",
      "Lucas: Such a success has to be celebrated.\n",
      "Demi: I agree! :D\n",
      "Demi: Tonight at Death & Co.?\n",
      "Lucas: Sure!\n",
      "Lucas: See you there at 10pm?\n",
      "Demi: Yeah! See you there! :D\n",
      "\n",
      "Summary:\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/corpus\"\n",
    "\n",
    "train_data, test_data = utils.get_train_test_data(data_dir)\n",
    "\n",
    "# Take one example from the dataset and print it\n",
    "example_summary, example_dialogue = train_data.iloc[10]\n",
    "print(f\"Dialogue:\\n{example_dialogue}\")\n",
    "print(f\"\\nSummary:\\n{example_summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bb560-e37c-4802-8902-fe9ce238e633",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "First you will do some preprocessing of the data and split it into inputs and outputs. Here you also remove some of the characters that are specific to this dataset and add the `[EOS]` (end of sentence) token to the end, like it was discussed in the lecture videos. You will also add a `[SOS]` (start of sentence) token to the beginning of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6116d6e0-a4ed-4c83-b863-ac8e7c750705",
   "metadata": {},
   "outputs": [],
   "source": [
    "document, summary           = utils.preprocess(train_data)\n",
    "document_test, summary_test = utils.preprocess(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2317423c-850c-430f-8fa5-b8151b1f658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6585c5aa-62a7-436c-9c4b-92e7e1bb1992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [SOS] amanda: i baked  cookies. do you want so...\n",
       "1        [SOS] olivia: who are you voting for in this e...\n",
       "2        [SOS] tim: hi, what's up?  kim: bad mood tbh, ...\n",
       "3        [SOS] edward: rachel, i think i'm in ove with ...\n",
       "4        [SOS] sam: hey  overheard rick say something  ...\n",
       "                               ...                        \n",
       "14727    [SOS] romeo: you are on my â€˜people you may kno...\n",
       "14728    [SOS] theresa: <file_photo>  theresa: <file_ph...\n",
       "14729    [SOS] john: every day some bad news. japan wil...\n",
       "14730    [SOS] jennifer: dear celia! how are you doing?...\n",
       "14731    [SOS] georgia: are you ready for hotel hunting...\n",
       "Name: dialogue, Length: 14732, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2e86871a-f755-4739-944f-79f544dd680a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d29a0900-0a54-4a83-a25d-c8912f0376d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\""
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d1d3b362-4e34-4da5-805d-63514e5f4b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [SOS] amanda baked cookies and will bring jerr...\n",
       "1        [SOS] olivia and olivier are voting for libera...\n",
       "2        [SOS] kim may try the pomodoro technique recom...\n",
       "3        [SOS] edward thinks he is in love with bella. ...\n",
       "4        [SOS] sam is confused, because he overheard ri...\n",
       "                               ...                        \n",
       "14727    [SOS] romeo is trying to get greta to add him ...\n",
       "14728    [SOS] theresa is at work. she gets free food a...\n",
       "14729    [SOS] japan is going to hunt whales again. isl...\n",
       "14730    [SOS] celia couldn't make it to the afternoon ...\n",
       "14731    [SOS] georgia and juliette are looking for a h...\n",
       "Name: summary, Length: 14732, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f62215-93bd-4f23-9a90-efca10fa4bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2869531d-4945-42d9-bb35-fb88c428c954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f611fbb9-d8e5-408c-92a2-952287cd72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c881e0-e28a-4e9e-bdf0-b6f6ad2a560d",
   "metadata": {},
   "source": [
    "Now perform the standard preprocessing with the tensorflow library. You will need to modify the filters, because you dont want the `[EOS]` tokens to be removed.\n",
    "\n",
    "Then create the vocabulary by combining the data in the documents and the summaries and using `.fit_on_texts()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6061540b-c4a4-4de5-8814-8252db3e48c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 34250\n"
     ]
    }
   ],
   "source": [
    "# The [ and ] from default tokens cannot be removed, because they mark the SOS and EOS token.\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n",
    "oov_token = '[UNK]'\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token, lower=False)\n",
    "\n",
    "documents_and_summary = pd.concat([document, summary], ignore_index=True)\n",
    "\n",
    "tokenizer.fit_on_texts(documents_and_summary)\n",
    "\n",
    "\n",
    "inputs  = tokenizer.texts_to_sequences(document)\n",
    "targets = tokenizer.texts_to_sequences(summary)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f'Size of vocabulary: {vocab_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e54efb-f24d-4662-8768-bff3f6c0344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4c66e25b-97b9-4fe4-8f0b-dcb760fbc19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [SOS] amanda: i baked  cookies. do you want so...\n",
       "1        [SOS] olivia: who are you voting for in this e...\n",
       "2        [SOS] tim: hi, what's up?  kim: bad mood tbh, ...\n",
       "3        [SOS] edward: rachel, i think i'm in ove with ...\n",
       "4        [SOS] sam: hey  overheard rick say something  ...\n",
       "                               ...                        \n",
       "29459    [SOS] romeo is trying to get greta to add him ...\n",
       "29460    [SOS] theresa is at work. she gets free food a...\n",
       "29461    [SOS] japan is going to hunt whales again. isl...\n",
       "29462    [SOS] celia couldn't make it to the afternoon ...\n",
       "29463    [SOS] georgia and juliette are looking for a h...\n",
       "Length: 29464, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_and_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "800efeec-cb87-4e0f-b1b0-4e2e327f9607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents_and_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5011b416-e198-4f5f-845d-6cb0becaac3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_and_summary[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "364420a0-5f2f-47a8-b61f-62df441c2180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_and_summary[14732]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0dee7b03-1e35-43d5-9b2f-32bda868f43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29464"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_and_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0b0eebc3-b062-42fb-96a4-99883c518d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "27d68e34-186b-4036-aa3c-dc45ee16d908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14732"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8650fbde-961b-441e-bc94-5c245b3e50f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 454, 2, 3500, 1611, 30, 5, 81, 50, 617, 66, 454, 63, 220, 5, 98, 8]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "36c90512-c032-4e43-94f3-f58e7526ae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fbf942ac-2205-4783-bca9-67a24bc38501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 454, 3500, 1611, 9, 15, 220, 617, 50, 98, 8]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f7efad-b4b2-4caa-97df-9adddadd0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42cabf-fdb4-4b02-951d-b7e351e8b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ea184cb1-d9d7-4c7f-bcd0-e6a206b86131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400f5c0-2716-4443-9a5a-35b9abc2ba01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3ff490cc-5923-4228-8795-ac00703af4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the size of the input and output data for being able to run it in this environment.\n",
    "encoder_maxlen = 150\n",
    "decoder_maxlen = 50\n",
    "\n",
    "# Pad the sequences.\n",
    "inputs  = tf.keras.preprocessing.sequence.pad_sequences(inputs,  maxlen=encoder_maxlen,  padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen,  padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bfbfa995-7a47-4de5-aaf0-719ccd2d5e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14732"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e12128dd-4376-4c74-94a9-51b7c95ab400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   7,  454,    2, ...,    0,    0,    0],\n",
       "       [   7,  339,  174, ...,    0,    0,    0],\n",
       "       [   7,  238,  116, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   7,  109,  462, ...,    0,    0,    0],\n",
       "       [   7,  632,  668, ...,  107,  312,   29],\n",
       "       [   7, 1667,   20, ...,  919,  108,    3]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 ko as a padding use kar rahay hain\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "be8973c0-6406-4182-be82-959131a3c30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 8)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['[UNK]'], tokenizer.word_index['[SOS]'], tokenizer.word_index['[EOS]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a0ae0e85-9d24-4cb2-a4a7-9429c2585e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ans\\AppData\\Local\\Temp\\ipykernel_19996\\1785072125.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs  = torch.tensor(inputs,  dtype=torch.int32)\n",
      "C:\\Users\\Ans\\AppData\\Local\\Temp\\ipykernel_19996\\1785072125.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# inputs  = torch.tensor(inputs,  dtype=torch.int32)\n",
    "# targets = torch.tensor(targets, dtype=torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7a25deb7-e593-4ab9-93c8-a2502e4b47bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   7,  454,    2,  ...,    0,    0,    0],\n",
       "        [   7,  339,  174,  ...,    0,    0,    0],\n",
       "        [   7,  238,  116,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   7,  109,  462,  ...,    0,    0,    0],\n",
       "        [   7,  632,  668,  ...,  107,  312,   29],\n",
       "        [   7, 1667,   20,  ...,  919,  108,    3]], dtype=torch.int32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ea96235b-9aae-4394-b577-e2c911e589f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9f4ad106-3187-4b0f-8e06-457e63ea834a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targets[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c0ae279-c545-4c02-ac41-afe029988755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aap chaaho tu ab in ka dataset use na kro, yaheen say apna khud ka generator define kr lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994c015-602b-4d45-9e4a-f823e96fa112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dfcab898-2bdc-4234-a4f3-7d1ee9554726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final training dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE  = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8d96724c-f73d-429b-a81a-baaa4edec654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 150), dtype=tf.int32, name=None), TensorSpec(shape=(None, 50), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "# notice the datatype, it is tf, so we'll have to convert it to torch before feeding it into our network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fe0bfe8f-8dc9-445e-b2ab-dbbafa264963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 156.25, 230.1875)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), 10000/64, (len(document))/64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327ea8a-ee8a-4e1f-89cb-18f1533e9b54",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model. However, when you train a Transformer network using multi-head attention, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful.\n",
    "\n",
    "You have learned how to implement the positional encoding in one of this week's labs. Here you will use the `positional_encoding` function to create positional encodings for your transformer. The function is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "921cf417-d6e7-4b7e-93f9-5cb4d19df4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int): Maximum number of positions to be encoded \n",
    "        d_model (int):   Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding (torch.tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
    "    \"\"\"\n",
    "    \n",
    "    position = np.arange(positions)[:, np.newaxis]\n",
    "    k = np.arange(d_model)[np.newaxis, :]\n",
    "    i = k // 2\n",
    "    \n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
    "    angle_rads = position * angle_rates\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "29e1d27d-e1fe-4565-bc62-41b9072e479b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 128])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(15,128).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce269726-e24b-48e6-9998-1654fb9a12a5",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba7c42-6fa0-46fa-a07f-b37046344c85",
   "metadata": {},
   "source": [
    "### Padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6323fbc-9159-4296-9844-5c84f69a54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids (matrix like): matrix of size (n, m)\n",
    "    \n",
    "    Returns:\n",
    "        mask : binary tensor of size (n, m)\n",
    "    \"\"\"\n",
    "    # if decoder_token_ids.shape[1] <= 1:\n",
    "    #     mask = None\n",
    "    # else:\n",
    "    mask = decoder_token_ids == 0\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdb1787f-4b5b-4b22-ae40-4f7b339ae9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    7,   454,     2,  3500,  1611,    30,     5,    81,    50,   617,\n",
       "            66,   454,    63,   220,     5,    98,     8,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    7,   339,   174,    20,     5,  5566,    12,    13,    38,  3018,\n",
       "           636, 12283,    59,   171,   339,    24,    69,   636,    99,     8,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:2,:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "63da6e9d-cd39-42d3-9d01-89f7a391862b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_padding_mask(inputs[:2,:30])\n",
    "# perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9ff16493-c6e0-4161-bde6-c9ccc0fcb65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), tensor([[7]]), tensor([[False]]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[7]])\n",
    "a.shape, a, create_padding_mask(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5cf7f-5836-4274-8ac9-ad1d1b21a5cf",
   "metadata": {},
   "source": [
    "# look ahead mask / attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9702abd-4301-46fd-9e1e-6c7f4bde88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow implementation\n",
    "def tf_create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        sequence_length (int): matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask (tf.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    return mask \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f31bd9e8-1b0a-40f8-b6f0-6c5d85e19926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_create_look_ahead_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a937859f-df43-478f-89c7-8d2a9eccbd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow implementation\n",
    "def create_look_ahead_mask(matrix_of_sequences):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        sequence_length (int): matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask (torch.tensor): binary tensor of size (sequence_length, sequence_length)\n",
    "    \"\"\"\n",
    "\n",
    "    # if matrix_of_sequences.shape[1] <= 1:\n",
    "    #     mask = None\n",
    "    # else:\n",
    "    mask = torch.triu(torch.ones(matrix_of_sequences.shape[1], matrix_of_sequences.shape[1]), diagonal=1).to(torch.bool)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fd4cd6f8-99a3-4f02-a82d-637951c07579",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8907a407-0f4a-4add-96a7-6260f284bc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = create_look_ahead_mask(b)\n",
    "a.shape\n",
    "# perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "002c74c9-205b-4a80-b22b-a2546fd8c217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c42fc2ca-fb1b-4b0d-8142-7fee24e2dba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), tensor([[7]]), tensor([[False]]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[7]])\n",
    "a.shape, a, create_look_ahead_mask(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "21dc6a10-74bc-4d0b-a3ca-c7224db77942",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "36f0ddf2-9b36-47df-9bda-5a55b73272aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9c97f479-4df6-41cd-ac3a-e9d26c4087b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "21a29502-fe88-41b7-a922-d11ec12aa006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(3, 15), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1c7a91ad-4921-468c-a164-d0a2b745f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c61c1-d83f-4f44-bb11-671d77b59f7c",
   "metadata": {},
   "source": [
    "Excellent work! You can now implement self-attention. With that, you can start building the encoder block! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bede9f-737c-48ad-8890-718b6b165bfe",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Encoder\n",
    "\n",
    "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you'll build later in the assignment. In this section of the assignment, you will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a). \n",
    "<img src=\"images/encoder_layer.png\" alt=\"Encoder\" width=\"400\"/>\n",
    "<caption><center><font color='purple'><b>Figure 2a: Transformer encoder layer</font></center></caption>\n",
    "\n",
    "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features. \n",
    "* Feed forward neural network contains two Dense layers.\n",
    "\n",
    "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62afa9a-924f-490b-a405-85081f80f9e7",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e6c281a4-50cf-43af-9394-fa3f9b62cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLayer(\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (layernorm1): BatchNorm1d(128, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=128, out_features=150, bias=True)\n",
      "  (fc2): Linear(in_features=150, out_features=128, bias=True)\n",
      "  (layernorm2): BatchNorm1d(128, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim_, num_heads_, fully_connected_dim_,  dropout_rate_=0.1, layernorm_eps_=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attention           = nn.MultiheadAttention(embed_dim=embedding_dim_, num_heads=num_heads_, dropout=dropout_rate_, batch_first=True)\n",
    "        self.layernorm1          = nn.BatchNorm1d(num_features=embedding_dim_, eps=layernorm_eps_)\n",
    "\n",
    "        self.fc1                 = nn.Linear(in_features=embedding_dim_, out_features=fully_connected_dim_)\n",
    "        self.fc2                 = nn.Linear(in_features=fully_connected_dim_, out_features=embedding_dim_)\n",
    "        self.layernorm2          = nn.BatchNorm1d(num_features=embedding_dim_, eps=layernorm_eps_)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate_)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: (batch_size, seq_length, embedding_dim)\n",
    "        # self attention\n",
    "        attention_output, attention_weights = self.attention(query=x, key=x, value=x, key_padding_mask=mask)  # Self attention (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # skip_connection\n",
    "        skip_x_attention = x + attention_output                   # (batch_size, seq_length, embedding_dim)\n",
    "        skip_x_attention = skip_x_attention.permute(0, 2, 1)      # (batch_size, embedding_dim, input_seq_len)\n",
    "        skip_x_attention = self.layernorm1(skip_x_attention)      # (batch_size, embedding_dim, input_seq_len)\n",
    "        skip_x_attention = skip_x_attention.permute(0, 2, 1)      # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # Dense / Fully connected layers / Feed forward neural network\n",
    "        fc1_output = self.fc1(skip_x_attention)\n",
    "        fc1_output = torch.relu(fc1_output)\n",
    "\n",
    "        fc2_output = self.fc2(fc1_output)\n",
    "        fc2_output = torch.relu(fc2_output)        \n",
    "\n",
    "        # dropout\n",
    "        fc2_output = self.dropout(fc2_output)\n",
    "\n",
    "        # 2nd skip connection\n",
    "        skip_attention_fc = skip_x_attention + fc2_output\n",
    "        skip_attention_fc = skip_attention_fc.permute(0, 2, 1)\n",
    "        skip_attention_fc = self.layernorm2(skip_attention_fc)\n",
    "        skip_attention_fc = skip_attention_fc.permute(0, 2, 1)\n",
    "\n",
    "        return skip_attention_fc\n",
    "\n",
    "enc_layer = EncoderLayer(\n",
    "    128,\n",
    "    2,\n",
    "    150,\n",
    "    dropout_rate_=0.1,\n",
    "    layernorm_eps_=1e-06,\n",
    ").to(device)\n",
    "print(enc_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9462b6a7-a972-4233-a1e5-5e223ff342f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfdce8a-53a5-41fe-b0ce-509adfdf7e24",
   "metadata": {},
   "source": [
    "### Checking Encoder's outputs layer by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b8b637fa-e874-4177-b06e-5da2521f1f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 15]),\n",
       " tensor([-1.4355, -1.6055, -1.2522,  0.7274,  1.3627,  1.0898,  1.8431,  1.2993,\n",
       "          0.7818,  0.2476, -0.8770, -1.4970, -0.0961,  1.5455,  0.4793],\n",
       "        device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor(2.6134, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(4.7684e-07, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of using BatchNorm1d\n",
    "batch_size   = 10\n",
    "num_features = 15\n",
    "\n",
    "# Initialize the BatchNorm1d layer\n",
    "batch_norm = nn.BatchNorm1d(num_features).to(device)\n",
    "\n",
    "# Example input tensor of shape (batch_size, num_features)\n",
    "input_tensor = torch.randn(batch_size, num_features).to(device)\n",
    "\n",
    "# Pass the input through the BatchNorm1d layer\n",
    "output = batch_norm(input_tensor)\n",
    "\n",
    "output.shape, output[0,:], torch.sum(output[0,:]), torch.sum(output[:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b6ba7839-eed1-42c7-ba30-dc62679855b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor torch.Size([10, 15])\n"
     ]
    }
   ],
   "source": [
    "# Example input tensor of shape (batch_size, num_features)\n",
    "input_tensor = torch.randint(0, 600, (batch_size, num_features)).to(device)\n",
    "print('input_tensor', input_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "360a1b2d-b872-4d47-9fac-ff00cf460a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_out:  torch.Size([10, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "embed_l   = nn.Embedding(num_embeddings=600, embedding_dim=128, padding_idx=0).to(device)\n",
    "embed_out = embed_l(input_tensor)\n",
    "print('embed_out: ', embed_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "91fbd8c9-ae43-4a94-a7c6-9d7e43f8a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_output:  torch.Size([10, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "attention_l  = nn.MultiheadAttention(embed_dim=128, num_heads=2, dropout=0.1, batch_first=True).to(device)\n",
    "mask         = create_padding_mask(input_tensor)\n",
    "\n",
    "# self attention\n",
    "attention_output, attention_weights = attention_l(query=embed_out, key=embed_out, value=embed_out, key_padding_mask=mask)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
    "print('attention_output: ', attention_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe3595-8b50-4e53-add0-0fe2e25f541b",
   "metadata": {},
   "source": [
    "#### [torch.nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)\n",
    "\n",
    "The explanation comes from ofiicial pytorch website\n",
    "Shape:\n",
    "\n",
    "        Input: (N,C)(N,C) or (N,C,L)(N,C,L), where NN is the batch size, CC is the number of features or channels, and LL is the sequence length\n",
    "\n",
    "        Output: (N,C)(N,C) or (N,C,L)(N,C,L) (same shape as input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2128e61e-37d7-4cf1-b9fe-313ff82a7f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one time permuted  x_plus_attn:  torch.Size([10, 128, 15])\n",
      "layernorm1_l_out:                torch.Size([10, 128, 15])\n",
      "two times permuted  x_plus_attn: torch.Size([10, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "x_plus_attn = attention_output + embed_out\n",
    "x_plus_attn = x_plus_attn.permute(0,2,1)\n",
    "print('one time permuted  x_plus_attn: ', x_plus_attn.shape)\n",
    "\n",
    "\n",
    "layernorm1_l  = nn.BatchNorm1d(num_features=128, eps=1e-6).to(device)\n",
    "x_plus_attn   = layernorm1_l(x_plus_attn)\n",
    "print(\"layernorm1_l_out:               \", x_plus_attn.shape)\n",
    "\n",
    "\n",
    "x_plus_attn = x_plus_attn.permute(0,2,1)\n",
    "print('two times permuted  x_plus_attn:', x_plus_attn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f718511b-f990-4cee-bfaf-dcf546ad037f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 15, 128]),\n",
       " tensor([ 2.1386,  0.9761, -0.5938,  0.4367,  2.2274,  0.5757,  1.2709, -1.7238,\n",
       "          1.2759, -0.2480,  1.6349, -0.5888,  0.5257, -1.4119, -0.6401],\n",
       "        device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor(3.1789e-09, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_plus_attn.shape, x_plus_attn[0,:,0], torch.mean(x_plus_attn[:,:,0]) # [:,:,0] : all the sequences, all the words of all the sequences, all the first feature's-values for all of the word's of all of the sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "aa49cf65-0209-478c-a811-490e47f16261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_out:  torch.Size([10, 15, 150])\n"
     ]
    }
   ],
   "source": [
    "fc1_l   = nn.Linear(in_features=128, out_features=150).to(device)\n",
    "fc1_out = fc1_l(x_plus_attn)\n",
    "fc1_out = torch.relu(fc1_out)\n",
    "\n",
    "print('fc1_out: ', fc1_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0634075c-b052-452a-a246-005d3a220c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2_out:  torch.Size([10, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "fc2_l   = nn.Linear(in_features=150, out_features=128).to(device)\n",
    "fc2_out = fc2_l(fc1_out)\n",
    "fc2_out = torch.relu(fc2_out)\n",
    "\n",
    "print('fc2_out: ', fc2_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4237530a-1fe4-4d9f-92ab-59f8348d94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droput_out:  torch.Size([10, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "dropout_l  = nn.Dropout(0.1).to(device)\n",
    "droput_out = dropout_l(fc2_out)\n",
    "print('droput_out: ', droput_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d86acff3-583e-43fc-bf62-9048b529ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# & so on .... 2nd skip connection and second normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7dd86-e613-44cc-a4b2-4d0351da6a28",
   "metadata": {},
   "source": [
    "#### Output Directly from EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "50ddf74b-6494-43a1-b234-6c597c88f3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_out:  torch.Size([10, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "enc_out = enc_layer(embed_out, create_padding_mask(input_tensor))\n",
    "print('enc_out: ', enc_out.shape)\n",
    "# perfect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a01eb92b-b182-443b-a0d6-f1a63d786cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49803cea-69d2-4664-8837-54a1ec370004",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - Full Encoder\n",
    "\n",
    "Now you're ready to build the full Transformer Encoder (Figure 2b), where you will embed your input and add the positional encodings you calculated. You will then feed your encoded embeddings to a stack of Encoder layers. \n",
    "\n",
    "<img src=\"images/encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "<caption><center><font color='purple'><b>Figure 2b: Transformer Encoder</font></center></caption>\n",
    "\n",
    "The Encoder class is implemented for you. It performs the following steps: \n",
    "1. Pass the input through the Embedding layer.\n",
    "2. Scale the embedding by multiplying it by the square root of the embedding dimension. \n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to the embedding.\n",
    "4. Pass the encoded embedding through a dropout layer\n",
    "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5bde071f-bac6-420d-9830-d2bd109222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"  \n",
    "    def __init__(self, num_layers_, embedding_dim_, num_heads_, fully_connected_dim_, input_vocab_size_, maximum_position_encoding_, dropout_rate_=0.1, layernorm_eps_=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim_\n",
    "        self.num_layers    = num_layers_\n",
    "\n",
    "        self.embedding    = nn.Embedding(num_embeddings=input_vocab_size_, embedding_dim=self.embedding_dim, padding_idx=0)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding_, self.embedding_dim)\n",
    "        \n",
    "\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(embedding_dim_=self.embedding_dim,\n",
    "                                        num_heads_=num_heads_,\n",
    "                                        fully_connected_dim_=fully_connected_dim_,\n",
    "                                        dropout_rate_=dropout_rate_,\n",
    "                                        layernorm_eps_=layernorm_eps_) \n",
    "                           for _ in range(self.num_layers)])\n",
    "\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate_)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, current_batch_of_sequences):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            current_batch_of_sequences (torch.tensor):    Tensor of shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            x (torch.tensor): Tensor of shape (batch_size, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        mask    = create_padding_mask(current_batch_of_sequences).to(current_batch_of_sequences.device)\n",
    "        seq_len = current_batch_of_sequences.shape[1]\n",
    "        \n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(current_batch_of_sequences)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        \n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x = x * torch.sqrt( torch.tensor(self.embedding_dim, dtype=torch.float32).to(x.device) )\n",
    "                                                                                            # x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        \n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :].to(x.device)\n",
    "\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "encoder_pseudo = Encoder(\n",
    "                        num_layers_                = 3,\n",
    "                        embedding_dim_             = 128,\n",
    "                        num_heads_                 = 2,\n",
    "                        fully_connected_dim_       = 150,\n",
    "                        input_vocab_size_          = 400,\n",
    "                        maximum_position_encoding_ = 15,\n",
    "                        dropout_rate_              = 0.1,\n",
    "                        layernorm_eps_             = 1e-06,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce1be3b9-c8bf-4dd2-b23e-0e5cffd856a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a3b7f-63ce-40a5-9eb6-05c047ae27c7",
   "metadata": {},
   "source": [
    "### Checking Full-Encoder's output layer by layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "121381aa-81e9-4de4-866f-4c7ed441d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ABC, self).__init__()\n",
    "\n",
    "    def forward(self, current_batch_of_sequences):\n",
    "        mask = create_padding_mask(current_batch_of_sequences)\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e2820c64-a79d-49d9-a1b2-e78eabd62d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_l = ABC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b05b6f38-5a39-4646-a06c-96e7e3bcb479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.create_padding_mask(decoder_token_ids)>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ab5e5cc9-a3ef-40c3-97b7-28deb6f0a90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True],\n",
       "        [False,  True,  True],\n",
       "        [False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,0],[1,0,0],[4,5,6]]).to(device)\n",
    "abc_l(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5f451e07-6ba6-426e-a433-c0bb284ff99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7321)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt( torch.tensor(a.shape[1]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7945f5cb-4aa9-4611-bf93-fd3ab3628cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7321,  3.4641,  0.0000],\n",
       "        [ 1.7321,  0.0000,  0.0000],\n",
       "        [ 6.9282,  8.6603, 10.3923]], device='cuda:0')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a * torch.sqrt( torch.tensor(a.shape[1], dtype=torch.float32).to(device))\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c2692aff-ef42-4a89-b4f9-6b71a050dc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "56178f4a-03f7-42e5-8d60-3b1121cc7d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 128])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "61f3d478-273c-40fc-a171-80a01d0b4abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 128])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(15, 128)\n",
    "pos_encoding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a356cac0-a744-4d38-8f7c-f2c974bd8701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 128])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automatic broadcasting\n",
    "n = embed_out + pos_encoding.to(device)\n",
    "n.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d825164-f4ad-46cc-bda4-6f60fa8b905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_layers = nn.ModuleList[EncoderLayer(\n",
    "#     128,\n",
    "#     2,\n",
    "#     150,\n",
    "#     dropout_rate_=0.1,\n",
    "#     layernorm_eps_=1e-06\n",
    "# ).to(device)\n",
    "#                    for _ in range(6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "708b1322-bc00-4046-b883-6c239aba401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_layers[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b747618c-5273-4242-ac56-6be84c23f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_layers[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa473ca1-d56b-4e0f-90db-8083c17444a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_layers[5](embed_out, mask).shape\n",
    "# # okayyy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcfa275-4df1-4c00-8be6-1fb4f8af6847",
   "metadata": {},
   "source": [
    "#### Direct output from full-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a36bde30-2113-4e3b-9955-41e55533f29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor torch.Size([10, 15])\n"
     ]
    }
   ],
   "source": [
    "# Example input tensor of shape (batch_size, num_features)\n",
    "input_tensor = torch.randint(0, 400, (batch_size, num_features)).to(device)\n",
    "print('input_tensor', input_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a171148d-33c4-410b-953e-c8889c22bde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 128])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_pseudo(input_tensor).shape\n",
    "# it is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc49e04f-30b3-43de-b089-9f4eb2280914",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c70817-9902-426e-990c-1e4d9a736832",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Decoder\n",
    "\n",
    "Now it is time to implement the decoder. You have seen it in the videos and you can use some help by looking at the encoder implementation above. The Decoder layer takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).\n",
    "\n",
    "<img src=\"images/decoder_layer.png\" alt=\"Decoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>Figure 3a: Transformer Decoder layer</font></center></caption>\n",
    "\n",
    "<a name='7-1'></a>    \n",
    "### 7.1 - Decoder Layer\n",
    "Again, you'll pair multi-head attention with a feed forward neural network, but this time you'll implement two multi-head attention layers. You will also use residual connections and layer normalization to help speed up training (Figure 3a).\n",
    "\n",
    "<a name='ex-2'></a>    \n",
    "### Exercise 2 - DecoderLayer\n",
    "    \n",
    "Implement `DecoderLayer()` using the `forward()` method\n",
    "    \n",
    "1. Block 1 is a multi-head attention layer with a residual connection, and look-ahead mask. Like in the `EncoderLayer`, Dropout is defined within the multi-head attention layer.\n",
    "2. Block 2 will take into account the output of the Encoder, so the multi-head attention layer will receive K and V from the encoder, and Q from the Block 1. You will then apply a normalization layer and a residual connection, just like you did before with the `EncoderLayer`.\n",
    "3. Finally, Block 3 is a feed forward neural network with dropout and normalization layers and a residual connection.\n",
    "    \n",
    "**Additional Hints:**\n",
    "* The first two blocks are fairly similar to the EncoderLayer except you will return `attention_scores` when computing self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5ac2d053-9879-492f-8246-e9bf1737516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DecoderLayer\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)        \n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True) \n",
    "\n",
    "        self.fc1  = nn.Linear(in_features=embedding_dim, out_features=fully_connected_dim)\n",
    "        self.fc2  = nn.Linear(in_features=fully_connected_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.layernorm1 = nn.BatchNorm1d(num_features=embedding_dim, eps=layernorm_eps)\n",
    "        self.layernorm2 = nn.BatchNorm1d(num_features=embedding_dim, eps=layernorm_eps)\n",
    "        self.layernorm3 = nn.BatchNorm1d(num_features=embedding_dim, eps=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask_dec_query, padding_mask_enc_key):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x (torch.tensor):               Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output (torch.tensor):      Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            look_ahead_mask (torch.tensor): Boolean mask for the target_input\n",
    "            padding_mask (torch.tensor):    Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 (torch.tensor):                Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attn_weights_block1 (torch.tensor): Tensor of shape (batch_size, ..............................\n",
    "            attn_weights_block2 (torch.tensor): Tensor of shape (batch_size, ..............................\n",
    "        \"\"\"\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # enc_output.shape == (batch_size, input_seq_len, fully_connected_dim) embeddin_dim\n",
    "        \n",
    "        # BLOCK 1\n",
    "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
    "        # Dropout will be applied during training (~1 line).\n",
    "        # if look_ahead_mask != None:\n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(query=x, key=x, value=x,\n",
    "                                                        is_causal        = True,               ##################################################\n",
    "                                                        attn_mask        = look_ahead_mask,\n",
    "                                                        key_padding_mask = padding_mask_dec_query)\n",
    "        # else:\n",
    "        #     mult_attn_out1, attn_weights_block1 = self.mha1(query=x, key=x, value=x)\n",
    "\n",
    "\n",
    "        # apply layer normalization (layernorm1) to the sum of the attention output and the input (~1 line)\n",
    "        # skip_connection\n",
    "        Q1 = x + mult_attn_out1\n",
    "        Q1 = Q1.permute(0, 2, 1)      # (batch_size, embedding_dim, input_seq_len)\n",
    "        Q1 = self.layernorm1(Q1)      # (batch_size, embedding_dim, input_seq_len)\n",
    "        Q1 = Q1.permute(0, 2, 1)      # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output. \n",
    "        # Dropout will be applied during training\n",
    "        # Return attention scores as attn_weights_block2 (~1 line) \n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(query=Q1, key=enc_output, value=enc_output, key_padding_mask=padding_mask_enc_key)\n",
    "\n",
    "        \n",
    "        # apply layer normalization (layernorm2) to the sum of the attention output and the output of the first block (~1 line)\n",
    "        mult_attn_out2 = Q1 + mult_attn_out2\n",
    "        mult_attn_out2 = mult_attn_out2.permute(0, 2, 1)      # (batch_size, embedding_dim, input_seq_len)\n",
    "        mult_attn_out2 = self.layernorm2(mult_attn_out2)      # (batch_size, embedding_dim, input_seq_len)\n",
    "        mult_attn_out2 = mult_attn_out2.permute(0, 2, 1)      # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        \n",
    "        #BLOCK 3\n",
    "        # pass the output of the multi-head attention layer through a ffn        \n",
    "        # Dense / Fully connected layers / Feed forward neural network\n",
    "        ffn_output = self.fc1(mult_attn_out2)\n",
    "        ffn_output = torch.relu(ffn_output)\n",
    "\n",
    "        ffn_output = self.fc2(ffn_output)\n",
    "        ffn_output = torch.relu(ffn_output)        \n",
    "\n",
    "        # dropout\n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "\n",
    "        \n",
    "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block        \n",
    "        out3 = ffn_output + mult_attn_out2\n",
    "        out3 = out3.permute(0, 2, 1)      # (batch_size, embedding_dim, input_seq_len)\n",
    "        out3 = self.layernorm3(out3)      # (batch_size, embedding_dim, input_seq_len)\n",
    "        out3 = out3.permute(0, 2, 1)      # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b3163006-805b-4414-820f-a30e772879e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding_dim=12 and num_heads=6:\n",
      "\n",
      "q has shape:torch.Size([1, 15, 12])\n",
      "Output of encoder has shape:torch.Size([1, 7, 12])\n",
      "\n",
      "Output of decoder layer has shape:torch.Size([1, 15, 12])\n",
      "Att Weights Block 1 has shape:torch.Size([1, 15, 15])\n",
      "Att Weights Block 2 has shape:torch.Size([1, 15, 7])\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "key_dim = 12\n",
    "n_heads = 6\n",
    "\n",
    "decoderLayer_test = DecoderLayer(embedding_dim=key_dim, num_heads=n_heads, fully_connected_dim=32).to(device)\n",
    "\n",
    "q = torch.from_numpy(np.ones((1, 15, key_dim))).to(torch.float32).to(device)\n",
    "encoder_test_output = torch.from_numpy(np.random.rand(1, 7, 12)).to(torch.float32).to(device)\n",
    "look_ahead_mask = create_look_ahead_mask(q).to(device)\n",
    "\n",
    "out, attn_w_b1, attn_w_b2 = decoderLayer_test(q, encoder_test_output, look_ahead_mask, None, None)\n",
    "\n",
    "print(f\"Using embedding_dim={key_dim} and num_heads={n_heads}:\\n\")\n",
    "print(f\"q has shape:{q.shape}\")\n",
    "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
    "\n",
    "print(f\"Output of decoder layer has shape:{out.shape}\")\n",
    "print(f\"Att Weights Block 1 has shape:{attn_w_b1.shape}\")\n",
    "print(f\"Att Weights Block 2 has shape:{attn_w_b2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19b0f674-c7bf-483f-bce9-3e3c503206c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e0832-8ee1-47a7-9ed9-6fd2a642d841",
   "metadata": {},
   "source": [
    "### Checking outputs of decoder-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e24f2fdc-213c-4c0e-852b-0aa92c48e465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 15]), torch.Size([10, 15, 128]))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, embed_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "09d75664-9c35-4ed0-8d0c-7caf75559856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mha1_out:  torch.Size([10, 15, 128])\n",
      "mha1_weights:  torch.Size([10, 15, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha1_l                 =  nn.MultiheadAttention(embed_dim=128, num_heads=2, dropout=0.1, batch_first=True).to('cpu')\n",
    "mha1_out, mha1_weights =  mha1_l(query=embed_out.to('cpu'), key=embed_out.to('cpu'), value=embed_out.to('cpu'),\n",
    "                                    is_causal        = True,\n",
    "                                    attn_mask        = create_look_ahead_mask(input_tensor).to('cpu'),\n",
    "                                    key_padding_mask = create_padding_mask(input_tensor).to('cpu'))\n",
    "\n",
    "print('mha1_out: ', mha1_out.shape), print('mha1_weights: ', mha1_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9ef21270-f3b9-4dd0-b24f-adb73eb748ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_look_ahead_mask(input_tensor)\n",
    "# create_padding_mask(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3dd12-5e1d-404b-881d-a5343437602f",
   "metadata": {},
   "source": [
    "#### Checking Decoder Layer's Output Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "508a5c7c-e3f3-415d-8895-1fa0f8c7f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderLayer_test = DecoderLayer(\n",
    "    embedding_dim=128,\n",
    "    num_heads=2,\n",
    "    fully_connected_dim=150,\n",
    "    dropout_rate=0.1,\n",
    "    layernorm_eps=1e-06,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "da187061-a20e-4204-bb44-9f30b5481daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  torch.Size([10, 15, 128])\n",
      "attn_w_b1:  torch.Size([10, 15, 15])\n",
      "attn_w_b2:  torch.Size([10, 15, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, attn_w_b1, attn_w_b2 = decoderLayer_test(embed_out.to('cpu'), embed_out.to('cpu'), \n",
    "                                              create_look_ahead_mask(input_tensor).to('cpu'),\n",
    "                                              create_padding_mask(input_tensor).to('cpu'), None)\n",
    "\n",
    "print('out: ', out.shape), print('attn_w_b1: ', attn_w_b1.shape), print('attn_w_b2: ', attn_w_b2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "72f645ed-676e-4c2f-9414-33f6df771834",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700550a7-74c7-4d83-ad35-9f0cc0e9ec1b",
   "metadata": {},
   "source": [
    "<a name='7-2'></a> \n",
    "### 7.2 - Full Decoder\n",
    "You're almost there! Time to use your Decoder layer to build a full Transformer Decoder (Figure 3b). You will embed your output and add positional encodings. You will then feed your encoded embeddings to a stack of Decoder layers. \n",
    "\n",
    "\n",
    "<img src=\"images/decoder.png\" alt=\"Decoder\" width=\"300\"/>\n",
    "<caption><center><font color='purple'><b>Figure 3b: Transformer Decoder</font></center></caption>\n",
    "\n",
    "<a name='ex-3'></a>     \n",
    "### Exercise 3 - Decoder\n",
    "\n",
    "Implement `Decoder()` using the `forward()` method to embed your output, add positional encoding, and implement multiple decoder layers.\n",
    " \n",
    "In this exercise, you will initialize your Decoder with an Embedding layer, positional encoding, and multiple DecoderLayers. Your `forward()` method will perform the following steps: \n",
    "1. Pass your generated output through the Embedding layer.\n",
    "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `torch.float32` before computing the square root.\n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
    "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
    "5. Pass the output of the dropout layer through the stack of Decoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c7db10f9-7dec-474a-849a-8fadeafa4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Decoder\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=target_vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)])\n",
    "       \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, current_batch_of_sequences, enc_input, enc_output):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x (torch.tensor):               Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output (torch.tensor):      Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            look_ahead_mask (torch.tensor): Boolean mask for the target_input\n",
    "            padding_mask (torch.tensor):    Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x (torch.tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, .....................................\n",
    "        \"\"\"\n",
    "        padding_mask_dec_query = create_padding_mask(current_batch_of_sequences).to(current_batch_of_sequences.device)        \n",
    "        padding_mask_enc_key   = create_padding_mask(enc_input).to(current_batch_of_sequences.device)\n",
    "        look_ahead_mask        = create_look_ahead_mask(current_batch_of_sequences).to(current_batch_of_sequences.device)\n",
    "        seq_len                = current_batch_of_sequences.shape[1]\n",
    "\n",
    "        attention_weights = {}\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # create word embeddings \n",
    "        x = self.embedding(current_batch_of_sequences)\n",
    "        \n",
    "        # scale embeddings by multiplying by the square root of their dimension\n",
    "        x = x * torch.sqrt( torch.tensor(self.embedding_dim, dtype=torch.float32).to(x.device) )\n",
    "        \n",
    "        # add positional encodings to word embedding\n",
    "        x += self.pos_encoding [:, :seq_len, :].to(x.device)\n",
    "\n",
    "        # apply a dropout layer to x\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "\n",
    "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
    "            # of block 1 and 2 (~1 line)\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask_dec_query, padding_mask_enc_key)\n",
    "\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)]   = block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, fully_connected_dim)\n",
    "        return x, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9534ff5b-2aef-4aad-9c7d-ed70c72cfe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7e3b55b5-9607-4965-a1a2-de202e42414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_pseudo = Encoder(\n",
    "                        num_layers_                = 5,\n",
    "                        embedding_dim_             = 128,\n",
    "                        num_heads_                 = 2,\n",
    "                        fully_connected_dim_       = 150,\n",
    "                        input_vocab_size_          = 600,\n",
    "                        maximum_position_encoding_ = 7,\n",
    "                        dropout_rate_              = 0.1,\n",
    "                        layernorm_eps_             = 1e-06,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2048a692-f4af-4759-b100-257b7a3431ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7, 128])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_in = torch.randint(0,100, (10, 7)).to(device)\n",
    "enc_out = encoder_pseudo(enc_in)\n",
    "enc_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2154635c-ef59-4278-96c7-0a4d019b1c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "635a7d01-263f-4f3a-bc42-fc119c822bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_test = Decoder(\n",
    "    num_layers                = 5,\n",
    "    embedding_dim             = 128,\n",
    "    num_heads                 = 2,\n",
    "    fully_connected_dim       = 150,\n",
    "    target_vocab_size         = 600,\n",
    "    maximum_position_encoding = 15,\n",
    "    dropout_rate              = 0.1,\n",
    "    layernorm_eps             = 1e-06\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "506ca170-5595-4fde-afd8-87c182a8189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outd:  torch.Size([10, 15, 128])\n",
      "att_weights:  torch.Size([10, 15, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outd, att_weights = decoder_test(input_tensor, enc_in, enc_out)\n",
    "print('outd: ', outd.shape), print('att_weights: ', att_weights['decoder_layer1_block1_self_att'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95248ec4-b8fe-486f-afaa-09bff62a8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e13df9-212f-42c1-a736-fdeb664d88e9",
   "metadata": {},
   "source": [
    "<a name='8'></a> \n",
    "## 8 - Transformer\n",
    "\n",
    "Phew! This has been quite the assignment! Congratulations! You've done all the hard work, now it's time to put it all together.  \n",
    "\n",
    "<img src=\"images/transformer.png\" alt=\"Transformer\" width=\"550\"/>\n",
    "<caption><center><font color='purple'><b>Figure 4: Transformer</font></center></caption>\n",
    "    \n",
    "The flow of data through the Transformer Architecture is as follows:\n",
    "* First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:\n",
    "    - embedding and positional encoding of your input\n",
    "    - multi-head attention on your input\n",
    "    - feed forward neural network to help detect features\n",
    "* Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:\n",
    "    - embedding and positional encoding of the output\n",
    "    - multi-head attention on your generated output\n",
    "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
    "    - a feed forward neural network to help detect features\n",
    "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in your sequence.\n",
    "\n",
    "<a name='ex-4'></a> \n",
    "### Exercise 4 - Transformer\n",
    "\n",
    "Implement `Transformer()` using the `forward()` method\n",
    "1. Pass the input through the Encoder with the appropiate mask.\n",
    "2. Pass the encoder output and the target through the Decoder with the appropiate mask.\n",
    "3. Apply a linear transformation and a softmax to get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c31c3527-67d3-46b6-ab2e-5e8e9e93463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Transformer\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers_=num_layers,\n",
    "                               embedding_dim_=embedding_dim,\n",
    "                               num_heads_=num_heads,\n",
    "                               fully_connected_dim_=fully_connected_dim,\n",
    "                               input_vocab_size_=input_vocab_size,\n",
    "                               maximum_position_encoding_=max_positional_encoding_input,\n",
    "                               dropout_rate_=dropout_rate,\n",
    "                               layernorm_eps_=layernorm_eps)\n",
    "\n",
    "        \n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size, \n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = nn.Linear(in_features=embedding_dim, out_features=target_vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, input_sentence, output_sentence):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            input_sentence (torch.tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "                                          An array of the indexes of the words in the input sentence\n",
    "            output_sentence (torch.tensor): Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "                                          An array of the indexes of the words in the output sentence\n",
    "        Returns:\n",
    "            final_output (torch.tensor): The final output of the model\n",
    "            attention_weights (dict[str: torch.tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, .....................................................\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        # call self.encoder with the appropriate arguments to get the encoder output\n",
    "        enc_output = self.encoder(input_sentence)\n",
    "        \n",
    "        # call self.decoder with the appropriate arguments to get the decoder output\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, embedding_dim)\n",
    "        dec_output, attention_weights = self.decoder(output_sentence, input_sentence, enc_output)\n",
    "        \n",
    "        # pass decoder output through a linear layer and log_softmax (~1 line)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        final_output = torch.nn.functional.log_softmax(final_output, dim=-1)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "54c0a743-6498-4c6e-bdc4-1d2670649989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using num_layers=3, target_vocab_size=350 and num_heads=17:\n",
      "\n",
      "sentence_a has shape:torch.Size([1, 7])\n",
      "sentence_b has shape:torch.Size([1, 7])\n",
      "\n",
      "Output of transformer (summary) has shape:torch.Size([1, 7, 350])\n",
      "\n",
      "Attention weights:\n",
      "decoder_layer1_block1_self_att has shape:torch.Size([1, 7, 7])\n",
      "decoder_layer1_block2_decenc_att has shape:torch.Size([1, 7, 7])\n",
      "decoder_layer2_block1_self_att has shape:torch.Size([1, 7, 7])\n",
      "decoder_layer2_block2_decenc_att has shape:torch.Size([1, 7, 7])\n",
      "decoder_layer3_block1_self_att has shape:torch.Size([1, 7, 7])\n",
      "decoder_layer3_block2_decenc_att has shape:torch.Size([1, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "n_layers                       = 3\n",
    "emb_d                          = 34   \n",
    "n_heads                        = 17\n",
    "fully_connected_dim            = 8\n",
    "input_vocab_size               = 300\n",
    "target_vocab_size              = 350\n",
    "max_positional_encoding_input  = 12\n",
    "max_positional_encoding_target = 12\n",
    "\n",
    "model = Transformer(n_layers, \n",
    "    emb_d, \n",
    "    n_heads, \n",
    "    fully_connected_dim, \n",
    "    input_vocab_size, \n",
    "    target_vocab_size, \n",
    "    max_positional_encoding_input,\n",
    "    max_positional_encoding_target).to(device)\n",
    "\n",
    "# 0 is the padding value\n",
    "sentence_a = torch.from_numpy(np.array([[2, 3, 1, 3, 0, 0, 0]])).to(torch.int).to(device)\n",
    "sentence_b = torch.from_numpy(np.array([[1, 3, 4, 0, 0, 0, 0]])).to(torch.int).to(device)\n",
    "\n",
    "\n",
    "test_summary, att_weights = model(\n",
    "    sentence_a,\n",
    "    sentence_b\n",
    ")\n",
    "\n",
    "print(f\"Using num_layers={n_layers}, target_vocab_size={target_vocab_size} and num_heads={n_heads}:\\n\")\n",
    "print(f\"sentence_a has shape:{sentence_a.shape}\")\n",
    "print(f\"sentence_b has shape:{sentence_b.shape}\")\n",
    "\n",
    "print(f\"\\nOutput of transformer (summary) has shape:{test_summary.shape}\\n\")\n",
    "print(\"Attention weights:\")\n",
    "for name, tensor in att_weights.items():\n",
    "    print(f\"{name} has shape:{tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc662513-b7c7-4dcb-b33d-cd1964228706",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Using num_layers=3, target_vocab_size=350 and num_heads=17:\n",
    "\n",
    "sentence_a has shape:(1, 7)\n",
    "sentence_b has shape:(1, 7)\n",
    "\n",
    "Output of transformer (summary) has shape:(1, 7, 350)\n",
    "\n",
    "Attention weights:\n",
    "decoder_layer1_block1_self_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer1_block2_decenc_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer2_block1_self_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer2_block2_decenc_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer3_block1_self_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer3_block2_decenc_att has shape:(1, 17, 7, 7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c5bb6-4d36-498a-9730-1c48397c6114",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## 9 - Initialize the Model\n",
    "Now that you have defined the model, you can initialize and train it. First you can initialize the model with the parameters below. Note that generally these models are much larger and you are using a smaller version to fit this environment and to be able to train it in just a few minutes.\n",
    "\n",
    "The base model described in the original Transformer paper used `num_layers=6`, `embedding_dim=512`, and `fully_connected_dim=2048`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "70e4b88d-91b0-4951-90cd-973a19508cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "num_layers                 = 2\n",
    "embedding_dim              = 128\n",
    "fully_connected_dim        = 128\n",
    "num_heads                  = 2\n",
    "positional_encoding_length = 256              ##################### okay tu yani k max seq len 256 tak hum rakhain gay\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(\n",
    "    num_layers, \n",
    "    embedding_dim, \n",
    "    num_heads, \n",
    "    fully_connected_dim,\n",
    "    vocab_size, \n",
    "    vocab_size, \n",
    "    positional_encoding_length, \n",
    "    positional_encoding_length,\n",
    ").to(device)\n",
    "\n",
    "# Transformer(\n",
    "#     num_layers,\n",
    "#     embedding_dim,\n",
    "#     num_heads,\n",
    "#     fully_connected_dim,\n",
    "#     input_vocab_size,\n",
    "#     target_vocab_size,\n",
    "#     max_positional_encoding_input,\n",
    "#     max_positional_encoding_target,\n",
    "#     dropout_rate=0.1,\n",
    "#     layernorm_eps=1e-06,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32507c-cefd-4334-81d2-409824400c19",
   "metadata": {},
   "source": [
    "<a name='11'></a>\n",
    "## 11 - Summarization\n",
    "\n",
    "The last thing you will implement is inference. With this, you will be able to produce actual summaries of the documents. You will use a simple method called greedy decoding, which means you will predict one word at a time and append it to the output. You will start with an `[SOS]` token and repeat the word by word inference until the model returns you the `[EOS]` token or until you reach the maximum length of the sentence (you need to add this limit, otherwise a poorly trained model could give you infinite sentences without ever producing the `[EOS]` token.\n",
    "\n",
    "<a name='ex-5'></a> \n",
    "### Exercise 5 - next_word\n",
    "Write a helper function that predicts the next word, so you can use it to write the whole sentences. Hint: this is very similar to what happens in the train_step, but you have to set the training of the model to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "37765438-3473-45c1-8485-ad7cbb22afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('transformer_based_text_summarizer.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f0ac061e-6ef2-4423-95e7-45bd61c7a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: next_word\n",
    "def next_word(model, encoder_input, output):\n",
    "    \"\"\"\n",
    "    Helper function for summarization that uses the model to predict just the next word.\n",
    "    Arguments:\n",
    "        encoder_input (torch.tensor): Input data to summarize\n",
    "        output (torch.tensor): (incomplete) target (summary)\n",
    "    Returns:\n",
    "        predicted_id (tf.Tensor): The id of the predicted word\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the prediction of the next word with the transformer model\n",
    "    predictions, attention_weights = model(encoder_input, output)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    predictions  = predictions[: ,-1:, :]\n",
    "    predicted_id = torch.argmax(predictions, axis=-1).to(torch.int32)\n",
    "    \n",
    "    return predicted_id\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b3355-f7f4-4176-bc11-8f71d2232dd1",
   "metadata": {},
   "source": [
    "Check if your function works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "31c03549-0298-4ad4-b3e8-44ad4832f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: tensor([[25543]], dtype=torch.int32)\n",
      "Predicted word: clasess\n"
     ]
    }
   ],
   "source": [
    "# Take a random sentence as an input\n",
    "input_document = tokenizer.texts_to_sequences([\"a random sentence\"])\n",
    "input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "encoder_input  = tf.expand_dims(input_document[0], 0).numpy()\n",
    "\n",
    "encoder_input = torch.from_numpy(encoder_input).to(torch.int32).to('cpu')\n",
    "\n",
    "\n",
    "# Take the start of sentence token as the only token in the output to predict the next word\n",
    "output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0).numpy()\n",
    "output = torch.from_numpy(output).to(torch.int32).to('cpu')\n",
    "\n",
    "model.eval()\n",
    "# predict the next word with your function\n",
    "predicted_token = next_word(model.to('cpu'), encoder_input, output)\n",
    "print(f\"Predicted token: {predicted_token}\")\n",
    "\n",
    "predicted_word = tokenizer.sequences_to_texts(predicted_token.to('cpu').numpy())[0]\n",
    "print(f\"Predicted word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e5070b7d-25b8-4320-860d-38b2be3efe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "def summarize(model, input_document):\n",
    "    \"\"\"\n",
    "    A function for summarization using the transformer model\n",
    "    Arguments:\n",
    "        input_document (tf.Tensor): Input data to summarize\n",
    "    Returns:\n",
    "        _ (str): The summary of the input_document\n",
    "    \"\"\"    \n",
    "    input_document = tokenizer.texts_to_sequences([input_document])\n",
    "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "    encoder_input  = tf.expand_dims(input_document[0], 0).numpy()\n",
    "    encoder_input  = torch.tensor(encoder_input).to(torch.int32).to(device)\n",
    "    \n",
    "    output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0).numpy()\n",
    "    output = torch.tensor(output).to(torch.int32).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    for i in range(decoder_maxlen):\n",
    "\n",
    "        predicted_id = next_word(model, encoder_input, output)\n",
    "        output       = torch.cat((output, predicted_id), dim=-1)\n",
    "        \n",
    "        if predicted_id == tokenizer.word_index[\"[EOS]\"]:\n",
    "            break\n",
    "\n",
    "    return tokenizer.sequences_to_texts(output.to('cpu').numpy())[0]  # since there is just one translated document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5362e-5343-4514-b0bc-95d334cd6f62",
   "metadata": {},
   "source": [
    "Now you can already summarize a sentence! But beware, since the model was not yet trained at all, it will just produce nonsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "88cc4e4d-2fa2-492c-b933-7613df1a11ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set example:\n",
      "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
      "\n",
      "Model written summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[SOS] amanda baked cookies and amanda will bring some cookies tomorrow [EOS]'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_example = 0\n",
    "\n",
    "# Check a summary of a document from the training set\n",
    "print('Training set example:')\n",
    "print(document[training_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary[training_set_example])\n",
    "print('\\nModel written summary:')\n",
    "summarize(model, document[training_set_example])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c80f6-9530-4b8a-9dc0-e81cc4aa9aef",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "cf76da88-be9f-4ff0-89f9-5f3e3174104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch, (inp, tar)) in enumerate(dataset):\n",
    "    if batch >=2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b0fdb24c-c0fc-42b2-a3d9-39a1f59d600d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 150]), torch.Size([64, 50]))"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.tensor(inp.numpy()).to(torch.int32).to(device)\n",
    "tar = torch.tensor(tar.numpy()).to(torch.int32).to(device)\n",
    "inp.shape, tar.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f0c8d02a-ce8f-4709-9a6f-887e1afa2df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   7, 2395,    5,  ...,    0,    0,    0],\n",
       "        [   7,  630,  307,  ...,    0,    0,    0],\n",
       "        [   7, 1894,   30,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   7, 9076,   17,  ...,    0,    0,    0],\n",
       "        [   7,  423, 1020,  ...,    0,    0,    0],\n",
       "        [   7,  399,   20,  ...,  125,  274,   34]], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a86da853-ae1f-4b53-af4b-188eac15d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds:  torch.Size([64, 50, 34250])\n"
     ]
    }
   ],
   "source": [
    "preds, _ = model(inp, tar)\n",
    "print('preds: ', preds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "8f2dcf9c-dc43-47d1-8320-7ba8fe70cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  torch.Size([3200, 34250])\n",
      "targets:  torch.Size([3200])\n",
      "10.202282905578613\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Ensure outputs is of type Float\n",
    "outputs = preds.float().clone()\n",
    "outputs = outputs.reshape(-1, preds.shape[2])\n",
    "print('outputs: ', outputs.shape)\n",
    "\n",
    "# Ensure targets is of type Long\n",
    "targets = tar.long().clone()\n",
    "targets = targets.reshape(-1)\n",
    "print('targets: ', targets.shape)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fd78a3a3-abd6-497e-911c-e268a523ce92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6f80e1d6-581b-4cf2-9b11-08c2b95be395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    7,   286,    70,  ...,     0,     0,     0],\n",
       "        [    7,   307,    15,  ...,     0,     0,     0],\n",
       "        [    7, 17173,    67,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    7,     3,  6944,  ...,     0,     0,     0],\n",
       "        [    7,   423,  5817,  ...,     0,     0,     0],\n",
       "        [    7,   399,    11,  ...,     0,     0,     0]], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "12f1dceb-3355-4040-a445-6672d48fbcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    7,   286,    70,  ...,     0,     0,     0],\n",
       "        [    7,   307,    15,  ...,     0,     0,     0],\n",
       "        [    7, 17173,    67,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    7,     3,  6944,  ...,     0,     0,     0],\n",
       "        [    7,   423,  5817,  ...,     0,     0,     0],\n",
       "        [    7,   399,    11,  ...,     0,     0,     0]], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar[:, :-1] # tar_inp | to decoder i guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a481e23b-0a03-4238-94bb-a606c619626f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  286,    70,  5217,  ...,     0,     0,     0],\n",
       "        [  307,    15,    21,  ...,     0,     0,     0],\n",
       "        [17173,    67,  3521,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    3,  6944,    33,  ...,     0,     0,     0],\n",
       "        [  423,  5817,   649,  ...,     0,     0,     0],\n",
       "        [  399,    11,  1387,  ...,     0,     0,     0]], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar[:, 1:] # tar_real | to loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "01eafff5-4cc6-4f35-a990-df1283fbca0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SOS]'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a87d88ba-8a63-4d4a-9c21-89adfb1d053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # error expected, explicitly hum nay hard code nahi kia tha, padding index\n",
    "# tokenizer.index_word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5b0b2-01ea-4f13-9312-1ea93e9860d4",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0f471a3d-19ff-4c60-afd8-68826c5bc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "num_layers                 = 2\n",
    "embedding_dim              = 128\n",
    "fully_connected_dim        = 128\n",
    "num_heads                  = 2\n",
    "positional_encoding_length = 256              ##################### okay tu yani k max seq len 256 tak hum rakhain gay\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(\n",
    "    num_layers, \n",
    "    embedding_dim, \n",
    "    num_heads, \n",
    "    fully_connected_dim,\n",
    "    vocab_size, \n",
    "    vocab_size, \n",
    "    positional_encoding_length, \n",
    "    positional_encoding_length,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "91e44a1c-ea60-4f1e-b865-9ea0aab27b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('transformer based summarizer.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "12094d83-79c1-43fd-8dd7-08ba11859f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10]\n",
      "\n",
      "20:  3.6275198578834535\n",
      "\n",
      "\n",
      "40:  3.676102066040039\n",
      "\n",
      "\n",
      "60:  3.722142791748047\n",
      "\n",
      "\n",
      "80:  3.754060631990433\n",
      "\n",
      "\n",
      "100:  3.7766186881065367\n",
      "\n",
      "\n",
      "120:  3.7964085976282758\n",
      "\n",
      "\n",
      "140:  3.818076319353921\n",
      "\n",
      "\n",
      "160:  3.8356647089123728\n",
      "\n",
      "\n",
      "180:  3.8478218966060216\n",
      "\n",
      "\n",
      "200:  3.8601041400432585\n",
      "\n",
      "\n",
      "220:  3.873857092857361\n",
      "\n",
      "Time taken for one epoch: 58.92952537536621 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda and hannah are going to the same place on the same place [EOS]\n",
      "\n",
      "Epoch [2 / 10]\n",
      "\n",
      "20:  3.5403417229652403\n",
      "\n",
      "\n",
      "40:  3.5622637033462525\n",
      "\n",
      "\n",
      "60:  3.579885896046956\n",
      "\n",
      "\n",
      "80:  3.5937197685241697\n",
      "\n",
      "\n",
      "100:  3.597118332386017\n",
      "\n",
      "\n",
      "120:  3.605500167608261\n",
      "\n",
      "\n",
      "140:  3.6236435277121406\n",
      "\n",
      "\n",
      "160:  3.6328338205814363\n",
      "\n",
      "\n",
      "180:  3.641001776854197\n",
      "\n",
      "\n",
      "200:  3.6505386698246003\n",
      "\n",
      "\n",
      "220:  3.658218042417006\n",
      "\n",
      "Time taken for one epoch: 58.73113822937012 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is going to the library amanda is on the library amanda is on the date with amanda is available hannah [EOS]\n",
      "\n",
      "Epoch [3 / 10]\n",
      "\n",
      "20:  3.4053343534469604\n",
      "\n",
      "\n",
      "40:  3.423880785703659\n",
      "\n",
      "\n",
      "60:  3.4313673893610637\n",
      "\n",
      "\n",
      "80:  3.437337189912796\n",
      "\n",
      "\n",
      "100:  3.4512291979789733\n",
      "\n",
      "\n",
      "120:  3.457466193040212\n",
      "\n",
      "\n",
      "140:  3.465772690091814\n",
      "\n",
      "\n",
      "160:  3.4769572734832765\n",
      "\n",
      "\n",
      "180:  3.491373368104299\n",
      "\n",
      "\n",
      "200:  3.502233054637909\n",
      "\n",
      "\n",
      "220:  3.5089281743223015\n",
      "\n",
      "Time taken for one epoch: 58.958091020584106 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah is going to the library [EOS]\n",
      "\n",
      "Epoch [4 / 10]\n",
      "\n",
      "20:  3.3044986367225646\n",
      "\n",
      "\n",
      "40:  3.28552251458168\n",
      "\n",
      "\n",
      "60:  3.299100462595622\n",
      "\n",
      "\n",
      "80:  3.3167436540126802\n",
      "\n",
      "\n",
      "100:  3.3311855363845826\n",
      "\n",
      "\n",
      "120:  3.3412631511688233\n",
      "\n",
      "\n",
      "140:  3.3496244737080167\n",
      "\n",
      "\n",
      "160:  3.359301343560219\n",
      "\n",
      "\n",
      "180:  3.3698523614141678\n",
      "\n",
      "\n",
      "200:  3.3797895491123198\n",
      "\n",
      "\n",
      "220:  3.3896132480014454\n",
      "\n",
      "Time taken for one epoch: 58.89878964424133 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda and hannah are meeting at the same place [EOS]\n",
      "\n",
      "Epoch [5 / 10]\n",
      "\n",
      "20:  3.1863906741142274\n",
      "\n",
      "\n",
      "40:  3.194918566942215\n",
      "\n",
      "\n",
      "60:  3.195246167977651\n",
      "\n",
      "\n",
      "80:  3.2126928180456162\n",
      "\n",
      "\n",
      "100:  3.231796476840973\n",
      "\n",
      "\n",
      "120:  3.2418917596340178\n",
      "\n",
      "\n",
      "140:  3.2578914182526724\n",
      "\n",
      "\n",
      "160:  3.267525868117809\n",
      "\n",
      "\n",
      "180:  3.2735331243938868\n",
      "\n",
      "\n",
      "200:  3.283470596075058\n",
      "\n",
      "\n",
      "220:  3.2927559982646595\n",
      "\n",
      "Time taken for one epoch: 58.961355447769165 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah is at the moment [EOS]\n",
      "\n",
      "Epoch [6 / 10]\n",
      "\n",
      "20:  3.0914679884910585\n",
      "\n",
      "\n",
      "40:  3.107124298810959\n",
      "\n",
      "\n",
      "60:  3.1244598547617595\n",
      "\n",
      "\n",
      "80:  3.1415715008974074\n",
      "\n",
      "\n",
      "100:  3.1451027750968934\n",
      "\n",
      "\n",
      "120:  3.153953194618225\n",
      "\n",
      "\n",
      "140:  3.1622799941471644\n",
      "\n",
      "\n",
      "160:  3.1710103690624236\n",
      "\n",
      "\n",
      "180:  3.1823562648561268\n",
      "\n",
      "\n",
      "200:  3.1935547149181365\n",
      "\n",
      "\n",
      "220:  3.2060713681307704\n",
      "\n",
      "Time taken for one epoch: 58.68945598602295 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda and amanda are going to the same place [EOS]\n",
      "\n",
      "Epoch [7 / 10]\n",
      "\n",
      "20:  3.024964225292206\n",
      "\n",
      "\n",
      "40:  3.0220491528511046\n",
      "\n",
      "\n",
      "60:  3.043429672718048\n",
      "\n",
      "\n",
      "80:  3.0499105781316755\n",
      "\n",
      "\n",
      "100:  3.0609449625015257\n",
      "\n",
      "\n",
      "120:  3.079175285498301\n",
      "\n",
      "\n",
      "140:  3.0880826813834052\n",
      "\n",
      "\n",
      "160:  3.1015374198555947\n",
      "\n",
      "\n",
      "180:  3.1139768242835997\n",
      "\n",
      "\n",
      "200:  3.1245123970508577\n",
      "\n",
      "\n",
      "220:  3.133977990800684\n",
      "\n",
      "Time taken for one epoch: 59.13540554046631 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda and hannah are going to meet on the 3rd floor [EOS]\n",
      "\n",
      "Epoch [8 / 10]\n",
      "\n",
      "20:  2.955716574192047\n",
      "\n",
      "\n",
      "40:  2.9620035529136657\n",
      "\n",
      "\n",
      "60:  2.972396739323934\n",
      "\n",
      "\n",
      "80:  2.9840184062719346\n",
      "\n",
      "\n",
      "100:  2.99915447473526\n",
      "\n",
      "\n",
      "120:  3.01210218667984\n",
      "\n",
      "\n",
      "140:  3.026995013441358\n",
      "\n",
      "\n",
      "160:  3.0392951890826225\n",
      "\n",
      "\n",
      "180:  3.051157049338023\n",
      "\n",
      "\n",
      "200:  3.0607543647289277\n",
      "\n",
      "\n",
      "220:  3.068506736105139\n",
      "\n",
      "Time taken for one epoch: 59.54641032218933 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda is looking for a good soak of his number amanda and she will meet with amanda [EOS]\n",
      "\n",
      "Epoch [9 / 10]\n",
      "\n",
      "20:  2.9046408891677857\n",
      "\n",
      "\n",
      "40:  2.9159153938293456\n",
      "\n",
      "\n",
      "60:  2.9277751525243123\n",
      "\n",
      "\n",
      "80:  2.9384529858827593\n",
      "\n",
      "\n",
      "100:  2.945161135196686\n",
      "\n",
      "\n",
      "120:  2.9569180727005007\n",
      "\n",
      "\n",
      "140:  2.9691358651433672\n",
      "\n",
      "\n",
      "160:  2.9804667234420776\n",
      "\n",
      "\n",
      "180:  2.9914680586920843\n",
      "\n",
      "\n",
      "200:  3.0014967727661133\n",
      "\n",
      "\n",
      "220:  3.0088628855618564\n",
      "\n",
      "Time taken for one epoch: 59.06548070907593 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] amanda and amanda are going to the library amanda will meet on the library amanda and amanda will be at 18 45 [EOS]\n",
      "\n",
      "Epoch [10 / 10]\n",
      "\n",
      "20:  2.856682300567627\n",
      "\n",
      "\n",
      "40:  2.849448227882385\n",
      "\n",
      "\n",
      "60:  2.8598528623580934\n",
      "\n",
      "\n",
      "80:  2.877936327457428\n",
      "\n",
      "\n",
      "100:  2.8927573800086974\n",
      "\n",
      "\n",
      "120:  2.9104459722836813\n",
      "\n",
      "\n",
      "140:  2.920953040463584\n",
      "\n",
      "\n",
      "160:  2.928695447742939\n",
      "\n",
      "\n",
      "180:  2.9374658981959025\n",
      "\n",
      "\n",
      "200:  2.9470205914974215\n",
      "\n",
      "\n",
      "220:  2.9578346122394907\n",
      "\n",
      "Time taken for one epoch: 58.83595037460327 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] hannah is getting married [EOS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "test_example  = 0\n",
    "true_summary  = summary_test[test_example]\n",
    "true_document = document_test[test_example]\n",
    "\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs    = 10 #20\n",
    "learning_rate = 0.001\n",
    "batch_size    = 64\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "pad_idx    = 0\n",
    "criterion  = nn.NLLLoss(ignore_index=pad_idx)\n",
    "optimizer  = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch [{epoch+1} / {num_epochs}]')\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "\n",
    "        inp1 = torch.tensor(inp.numpy()).long().clone().to(device)\n",
    "        tar1 = torch.tensor(tar.numpy()).long().clone().to(device)\n",
    "\n",
    "        # print('inp_data1: ', inp_data1.shape)\n",
    "        # print('target1: ',   target1.shape)\n",
    "\n",
    "        preds, _ = model(inp1, tar1[:, :-1])\n",
    "\n",
    "        # Ensure outputs is of type Float\n",
    "        outputs = preds.float().clone()\n",
    "        outputs = outputs.reshape(-1, preds.shape[2])\n",
    "        # print('outputs: ', outputs.shape)\n",
    "        \n",
    "        # Ensure targets is of type Long\n",
    "        targets = tar1[:, 1:].long().clone()\n",
    "        targets = targets.reshape(-1)\n",
    "        # print('targets: ', targets.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss = running_loss+loss.item()\n",
    "        avg_running_loss = running_loss/(batch+1)\n",
    "\n",
    "        if (batch+1)%20 == 0:\n",
    "            print()\n",
    "            print(f'{batch+1}: ', avg_running_loss)\n",
    "            print()\n",
    "        \n",
    "        if (batch+1) >= len(dataset):\n",
    "            break\n",
    "    \n",
    "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
    "    print('Example summarization on the test set:')\n",
    "    print('  True summarization:')\n",
    "    print(f'    {true_summary}')\n",
    "    print('  Predicted summarization:')\n",
    "    \n",
    "    model.eval()\n",
    "    print(f'    {summarize(model, true_document)}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7822bab2-341a-4483-bdca-5c526046fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'transformer_based_text_summarizer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e70a2d-825f-4575-929b-fa74205e83bf",
   "metadata": {},
   "source": [
    "<a name='13'></a>\n",
    "# 13 - Summarize some Sentences!\n",
    "\n",
    "Below you can see an example of summarization of a sentence from the training set and a sentence from the test set. See if you notice anything interesting about them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8cbfd59c-c126-4261-a5f1-3791c6d9a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('transformer_based_text_summarizer.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "68065d4a-626b-45c4-9d7b-8b58d50030d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set example:\n",
      "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] amanda baked cookies and amanda will bring some cookies tomorrow [EOS]\n"
     ]
    }
   ],
   "source": [
    "training_set_example = 0\n",
    "\n",
    "# Check a summary of a document from the training set\n",
    "print('Training set example:')\n",
    "print(document[training_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary[training_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(model, document[training_set_example]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "85ee027d-5ec7-422b-832e-e784f5fa0f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set example:\n",
      "[SOS] will: hey babe, what do you want for dinner tonight?  emma:  gah, don't even worry about it tonight  will: what do you mean? everything ok?  emma: not really, but it's ok, don't worry about cooking though, i'm not hungry  will: well what time will you be home?  emma: soon, hopefully  will: you sure? maybe you want me to pick you up?  emma: no no it's alright. i'll be home soon, i'll tell you when i get home.   will: alright, love you.   emma: love you too.  [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] emma will be home soon and she will let will know. [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] emma will be home in 15 minutes [EOS]\n"
     ]
    }
   ],
   "source": [
    "test_set_example = 3\n",
    "\n",
    "# Check a summary of a document from the test set\n",
    "print('Test set example:')\n",
    "print(document_test[test_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary_test[test_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(model, document_test[test_set_example]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30e967-1fb4-4fb3-ac66-c3b1eaf298d0",
   "metadata": {},
   "source": [
    "If you critically examine the output of the model, you can notice a few things:\n",
    " - In the training set the model output is (almost) identical to the real output (already after 20 epochs and even more so with more epochs). This might be because the training set is relatively small and the model is relatively big and has thus learned the sentences in the training set by heart (overfitting).\n",
    " - While the performance on the training set looks amazing, it is not so good on the test set. The model overfits, and fails to generalize. Again an easy candidate to blame is the small training set and a comparatively large model, but there might be a variety of other factors.\n",
    " - Look at the test set example 3 and its summarization. Would you summarize it the same way as it is written here? Sometimes the data may be ambiguous. And the training of **your model can only be as good as your data**.\n",
    "\n",
    "Here you only use a small dataset, to show that something can be learned in a reasonable amount of time in a relatively small environment. Generally, large transformers are trained on more than one task and on very large quantities of data to achieve superb performance. You will learn more about this in the rest of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014a692-97b8-40bd-b262-81301d4a069e",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this week's assignment!** You did a lot of work and now you should have a better understanding of the Transformers and their building blocks (encoder and decoder) and how they can be used for text summarization. And remember: you dont need to change much to use the same model for a translator, just change the dataset and it should work!\n",
    "\n",
    "**Keep it up!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "9cc303b4-7744-48d6-b38c-bdd318456c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set example:\n",
      "[SOS] he woke up at 5 am. he drank some water. he offered his prayer. he started to study. he completed his work. it was a productive day [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] heidi was at work [EOS]\n"
     ]
    }
   ],
   "source": [
    "test_set_example      = '[SOS] he woke up at 5 am. he drank some water. he offered his prayer. he started to study. he completed his work. it was a productive day [EOS]'\n",
    "\n",
    "# Check a summary of a document from the test set\n",
    "print('Test set example:')\n",
    "print(test_set_example)\n",
    "\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(model, test_set_example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af8f5d-5f81-4a08-8ad2-539c39eb5d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
